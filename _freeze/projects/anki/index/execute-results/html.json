{
  "hash": "67d43b3c4aa1ce8d75b49d356be142e9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Project: Anki flashcards\"\njupyter: python3\n---\n\n\n\n\n## Setup\n\nTo get started, you can clone the repository containing the project files:\n\n```bash\ngit clone https://github.com/awellis/anki-flashcard-generator\n```\n\nor simply download the zip file from my [Github repository](https://github.com/awellis/anki-flashcard-generator).\n\n\n![](../../assets/images/github-anki.png){width=80%}\n\n\nOnce you cloned the repository, or downloaded and unzipped the zip file, you will find the following files:\n\n```bash\nassets/baroque-essay.md\nassets/classical-essay.md\nassets/romantic-essay.md\nassets/modern-essay.md\n```\nThese files contain the teaching material for the four musical eras, based on which we will generate the flashcards. \n\n```bash\ngenerate-anki-flashcards.py\n```\nThis file contains a Python script to get you started.\n\n## Tasks\n\n::: {.try title=\"Task 1: Create an LLM client\"}\n\nFirst, you will to set up an LLM client. We will use the `openai` Python package to connect to an OpenAI model.\n\n(@) import the `openai` package\n(@) import the `dotenv` package to load the API key from the `.env` file\n(@) set up the client with your OpenAI API key\n:::\n\n::: {.try title=\"TTask 2: Read the teaching material\"}\n\n\n(@) read one of the essay files, and print the contents\n:::\n\n::: {.try title=\"Task 3: Extract pairs of questions and answers\"}\nNow you can think about how you can extract pairs of questions and answers from the teaching material. You will need to write a suitable prompt, consisting of a system message and a user message, to guide the LLM in extracting the questions and answers.\n\n(@) write a prompt to extract pairs of questions and answers from the teaching material\n:::\n\n::: {.try title=\"Task 4: Call the LLM\"}\n\n(@) call the LLM with the prompt, and print the result\n  + try out both GPT-4o and GPT-4o Mini\n  + try out different parameters settings for the LLM call (e.g. `temperature`, `top_p`)\n:::\n\n::: {.try title=\"Task 5: Use structured outputs\"}\n\n(@) use structured outputs to control the format of the LLM's response: Define a `pydantic` model to describe the format of the LLM's response\n(@) call the LLM with the structured output format\n:::\n\n::: {.try title=\"Task 6: Make your code reusable\"}\n\n(@) write a function to call the LLM with the prompt and an arbitrary text\n(@) call the function with the prompt, and print the result\n:::\n\n::: {.try title=\"Task 7: Write the results to a CSV file\"}\n\n(@) write the results to a CSV file. Code is provided for you.\n:::\n\n::: {.try title=\"Task 8: Extract questions and answers from all the teaching material\"}\n\n(@) write a loop to load all essays, extract questions and answers from all the teaching material, and write the results to a CSV file.\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}