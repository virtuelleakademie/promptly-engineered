[
  {
    "objectID": "workshop/index.html",
    "href": "workshop/index.html",
    "title": "Contents",
    "section": "",
    "text": "In this workshop, we will cover the following topics:\n\nUsing the OpenAI API\nCreating an Anki flashcard generator using the OpenAI API\n\nWe will work with the GPT-4o-mini and GPT-4o models from OpenAI. Note that we could also use local models, but this would require a bit more setup and is outside the scope of this workshop.\nThe focus of this workshop is on using the OpenAI API, so we will only discuss Python in passing. I do recommend that you install an AI coding assistant, such as Github Copilot. For beginners, this will help to get you started with Python.\nThe workshop is structured as follows:\n\nIntroduction\nSetup coding environment\n\nsetup Python\nsetup VSCode\ninstall Github Copilot\n\nVerify OpenAI installation\nUsing the OpenAI API\nProject: Use GPT-4o to generate Anki flashcards\nConclusions and discussion\n\n\n\n\n\n\n\n\n Back to topReuseCC BY 4.0CitationBibTeX citation:@online{ellis2024,\n  author = {Ellis, Andrew},\n  title = {Contents},\n  date = {2024-11-13},\n  url = {https://virtuelleakademie.github.io/promptly-engineered/workshop/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEllis, Andrew. 2024. ‚ÄúContents.‚Äù November 13, 2024. https://virtuelleakademie.github.io/promptly-engineered/workshop/.",
    "crumbs": [
      "Workshop",
      "Contents"
    ]
  },
  {
    "objectID": "workshop/openai/index.html",
    "href": "workshop/openai/index.html",
    "title": "Setup OpenAI",
    "section": "",
    "text": "Now that we have set up our Python environment, we can start using the OpenAI API.\nLet‚Äôs open the OpenAI Platform. Make sure that you are logged in. Here, we will first look at the OpenAI Playground and then we will create an API key. We need this key to use the OpenAI API from our Python code.",
    "crumbs": [
      "Workshop",
      "Setup OpenAI"
    ]
  },
  {
    "objectID": "workshop/openai/index.html#understanding-roles-in-the-messages-argument",
    "href": "workshop/openai/index.html#understanding-roles-in-the-messages-argument",
    "title": "Setup OpenAI",
    "section": "Understanding Roles in the messages Argument",
    "text": "Understanding Roles in the messages Argument\nWhen using the chat completions API, you create prompts by providing an array of messages that contain instructions for the model. Each message can have a different role, which influences how the model might interpret the input. Each entry in the messages list is a dictionary with a role and a content. The role specifies who is ‚Äúspeaking,‚Äù which helps the model generate contextually appropriate responses.",
    "crumbs": [
      "Workshop",
      "Setup OpenAI"
    ]
  },
  {
    "objectID": "workshop/openai/index.html#roles-and-their-purposes",
    "href": "workshop/openai/index.html#roles-and-their-purposes",
    "title": "Setup OpenAI",
    "section": "Roles and Their Purposes",
    "text": "Roles and Their Purposes\n\n1. system\n\nPurpose: Sets the behavior, tone, and personality of the AI. Think of this as the ‚Äúguiding principles‚Äù for the model.\nWhen to Use: At the beginning of a conversation to establish how the assistant should behave.\nExample:\n\n{\"role\": \"system\", \"content\": \"You are a helpful and polite assistant.\"}\n\nEffect:\n\nIt tells the model to frame all its responses according to the specified behavior.\nFor example, defining the assistant as ‚Äúconcise‚Äù will encourage brief replies.\n\n\n\n\n2. user\n\nPurpose: Represents the input from the person using the model.\nWhen to Use: Every time the user provides input or asks a question.\nExample:\n\n{\"role\": \"user\", \"content\": \"Can you explain the roles in the messages argument?\"}\n\nEffect:\n\nThe model treats this as a direct prompt to respond.\nThe user‚Äôs input frames the assistant‚Äôs reply.\n\n\n\n\n3. assistant\n\nPurpose: Represents the AI‚Äôs responses in the conversation.\nWhen to Use: To show the model what it has previously said, especially in multi-turn interactions.\nExample:\n\n{\"role\": \"assistant\", \"content\": \"Of course! Here‚Äôs an explanation of the roles...\"}\n\nEffect:\n\nBy including prior responses, you ensure the model has full context for the ongoing conversation.\n\n\n\n\n4. function (Optional, Advanced)\n\nPurpose: Represents a structured response when calling functions integrated with the AI.\nWhen to Use: In applications where the AI triggers external functions (e.g., retrieving weather data or performing calculations).\nExample:\n\n\n{\"role\": \"function\", \n \"name\": \"get_weather\", \n \"content\": \"{\\\"location\\\": \\\"Zurich\\\"}\"}\n\nEffect:\n\nUsed in function-calling mode to indicate what data or output the function provides.\n\n\nWe will not use the function role in this workshop.",
    "crumbs": [
      "Workshop",
      "Setup OpenAI"
    ]
  },
  {
    "objectID": "workshop/openai/index.html#putting-it-all-together",
    "href": "workshop/openai/index.html#putting-it-all-together",
    "title": "Setup OpenAI",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nHere‚Äôs an example of a complete messages argument in a conversational context:\n[\n    {\"role\": \"system\", \"content\": \"You are a friendly travel assistant.\"},\n    {\"role\": \"user\", \"content\": \"Can you suggest a good vacation spot for December?\"},\n    {\"role\": \"assistant\", \"content\": \"Sure! How about visiting the Swiss Alps for skiing?\"},\n    {\"role\": \"user\", \"content\": \"That sounds great. What else can I do there?\"}\n]\nBy structuring your prompts as an array of messages with different roles, you can have more control over the conversation flow and provide additional context or instructions to the model as needed.",
    "crumbs": [
      "Workshop",
      "Setup OpenAI"
    ]
  },
  {
    "objectID": "workshop/openai/index.html#suggestion-for-using-temperature-and-top-p",
    "href": "workshop/openai/index.html#suggestion-for-using-temperature-and-top-p",
    "title": "Setup OpenAI",
    "section": "Suggestion for using temperature and top-p",
    "text": "Suggestion for using temperature and top-p\nTop-p sampling, also known as nucleus sampling, is a way to control the diversity of the text generated by a language model. It works by considering the most likely words or tokens at each step, but instead of just taking the top few, it takes the smallest set of words that make up a certain percentage (p) of the total probability.\nFor example, if p is set to 0.9, the model will consider the words that make up the top 90% of the probability distribution at each step. This allows for more diverse and creative outputs compared to just taking the single most likely word.\nYou can use the temperature and top-p parameters in combination to make the LLM more creative or more focused.\nIncrease the temperature parameter to make the model‚Äôs outputs more diverse and creative. However, if the temperature is too high, the outputs may become nonsensical or incoherent. In that case, you can lower the top p value to restrict the model‚Äôs vocabulary and make the outputs more focused and coherent.\nIf you find that you need to lower the top-p value below 0.5 (or 50%) to keep the outputs coherent, it may be better to lower the temperature instead. Then, you can try adjusting the top-p value again to find the right balance between diversity and coherence.\nThe key is to experiment with different combinations of temperature and top-p values to achieve the desired level of creativity and coherence for your specific use case.",
    "crumbs": [
      "Workshop",
      "Setup OpenAI"
    ]
  },
  {
    "objectID": "workshop/openai/index.html#setting-the-api-key-as-an-environment-variable",
    "href": "workshop/openai/index.html#setting-the-api-key-as-an-environment-variable",
    "title": "Setup OpenAI",
    "section": "Setting the API key as an environment variable",
    "text": "Setting the API key as an environment variable\nIn your VSCode workspace, create a new file called .env. In this file, add the following line:\nOPENAI_API_KEY=&lt;your-api-key&gt;\nwhere &lt;your-api-key&gt; is the API key you copied in the previous step. It should look like this:\nOPENAI_API_KEY=sk-proj-...",
    "crumbs": [
      "Workshop",
      "Setup OpenAI"
    ]
  },
  {
    "objectID": "workshop/openai/index.html#testing-the-openai-api-with-a-jupyter-notebook",
    "href": "workshop/openai/index.html#testing-the-openai-api-with-a-jupyter-notebook",
    "title": "Setup OpenAI",
    "section": "Testing the OpenAI API with a Jupyter Notebook",
    "text": "Testing the OpenAI API with a Jupyter Notebook\nYou can also test the OpenAI API with a Jupyter Notebook. To do this, create a new Jupyter Notebook and insert the following code in individual cells.\nfrom dotenv import load_dotenv\nfrom openai import OpenAI \nload_dotenv()\nclient = OpenAI()\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n              {\"role\": \"user\", \"content\": \"What is the weather in Bern?\"}]\n        )\nprint(response)\nprint(response.choices[0].message.content)",
    "crumbs": [
      "Workshop",
      "Setup OpenAI"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Virtuelle Akademie",
    "section": "",
    "text": "QR Code\n\n\n\n virtuelleakademie.github.io/promptly-engineered\n\n\n\n Back to top"
  },
  {
    "objectID": "slides/template-slides.html#slide-1",
    "href": "slides/template-slides.html#slide-1",
    "title": "AI-Enhanced Journal Club",
    "section": "Slide 1",
    "text": "Slide 1"
  },
  {
    "objectID": "slides/template-slides.html#slide-2",
    "href": "slides/template-slides.html#slide-2",
    "title": "AI-Enhanced Journal Club",
    "section": "Slide 2",
    "text": "Slide 2\n                    \n                    \n                \n\n\n\n\nBerner Fachhochschule | Bern University of Applied Sciences"
  },
  {
    "objectID": "slides/prompt-engineering-intermediate/index.html#chain-of-thought-cot-prompting",
    "href": "slides/prompt-engineering-intermediate/index.html#chain-of-thought-cot-prompting",
    "title": "Prompt Engineering: Intermediate Techniques",
    "section": "Chain of Thought (CoT) prompting",
    "text": "Chain of Thought (CoT) prompting\nTechniques: Encourage the model to proceed in a step-by-step manner. This has the effect of making the desired output more probable. The output looks like the LLM is showing its reasoning process1.\n\n\n\n\n\n\nExample Prompt\n\n\nThink through this step-by-step: 1) List the symptoms 2) Consider possible causes 3) Evaluate urgency 4) Recommend action\n\n\n\nOften it can be sufficient to just ask the model to think step-by-step.\n\n\n\n\n\n\nExample Prompt\n\n\nThink step-by-step.\n\n\n\nThis behaviour has been trained into recent models."
  },
  {
    "objectID": "slides/prompt-engineering-intermediate/index.html#why-chain-of-thought",
    "href": "slides/prompt-engineering-intermediate/index.html#why-chain-of-thought",
    "title": "Prompt Engineering: Intermediate Techniques",
    "section": "Why Chain of Thought?",
    "text": "Why Chain of Thought?\n\nAmount of computation is constant per token.\nBy forcing the LLM to generate more (useful) tokens, it will therefore generate more (useful) content.\nThis in turn narrows the space of possible outputs, and steers the model towards regions of the output space that are more desirable."
  },
  {
    "objectID": "slides/prompt-engineering-intermediate/index.html#drawbacks-of-chain-of-thought",
    "href": "slides/prompt-engineering-intermediate/index.html#drawbacks-of-chain-of-thought",
    "title": "Prompt Engineering: Intermediate Techniques",
    "section": "Drawbacks of Chain of Thought",
    "text": "Drawbacks of Chain of Thought\n\nLLM performance on reasoning problems does not generalize well\nChain of thought prompting aims to mitigate this by demonstrating solution procedures\nStechly, Valmeekam, and Kambhampati (2024) found meaningful performance improvements only with highly problem-specific prompts."
  },
  {
    "objectID": "slides/prompt-engineering-intermediate/index.html#few-shot-learning",
    "href": "slides/prompt-engineering-intermediate/index.html#few-shot-learning",
    "title": "Prompt Engineering: Intermediate Techniques",
    "section": "Few-Shot Learning",
    "text": "Few-Shot Learning\nTechnique: Provide multiple examples before asking for a new output.\nThe way that we structure Few-Shot Prompts is very important. By this, we mean do we separate the inputs and outputs with a colon (:) or the words INPUT/OUTPUT. We have seen examples of both earlier in this article. How can you decide? We generally use the input: output format and occasionally use the QA format, which is commonly used in research papers.\nUse 2-5 examples for simple tasks. Use often ~10 examples for harder tasks\n\n\n\n\n\n\nExample Prompt\n\n\nInput: ‚ÄúGreat product, 10/10‚Äù\nOuput: ‚ÄúGreat product, 10/10‚Äù: {‚Äúlabel‚Äù: ‚Äúpositive‚Äù}\n Input: ‚ÄúDidn‚Äôt work very well‚Äù\nOutput: ‚ÄúDidn‚Äôt work very well‚Äù: {‚Äúlabel‚Äù: ‚Äúnegative‚Äù}\n Input: ‚ÄúSuper helpful, worth it‚Äù\nOutput: ‚ÄúSuper helpful, worth it‚Äù: {‚Äúlabel‚Äù: ‚Äúpositive‚Äù}\n Input: ‚ÄúI‚Äôm not sure I would buy this again‚Äù\nOutput:"
  },
  {
    "objectID": "slides/prompt-engineering-intermediate/index.html#structured-output",
    "href": "slides/prompt-engineering-intermediate/index.html#structured-output",
    "title": "Prompt Engineering: Intermediate Techniques",
    "section": "Structured Output",
    "text": "Structured Output\nTechnique: Specify a structure for the model‚Äôs response.\n\n\n\n\n\n\nExample Prompt\n\n\nProvide your assessment in JSON format:\n{\n  \"severity\": \"[Emergency/Urgent/Non-urgent]\",\n  \"potential_causes\": \"[List top 3]\",\n  \"recommended_action\": \"[Specific next steps]\"\n}"
  },
  {
    "objectID": "slides/prompt-engineering-intermediate/index.html#self-consistency",
    "href": "slides/prompt-engineering-intermediate/index.html#self-consistency",
    "title": "Prompt Engineering: Intermediate Techniques",
    "section": "Self-Consistency",
    "text": "Self-Consistency\nLLMs are prone to variability in their responses.\nTechnique: Generate multiple answers, aggregate the responses and select the majority result.\n\n\nDo this several times:\n\n\n\n\n\n\nExample Prompt\n\n\nProvide three independent assessments for these symptoms.\nThink step-by-step.\nSymptoms: [insert symptoms here]\n\n\n\n\nProvide the responses to an LLM in a new session:\n\n\n\n\n\n\nExample Prompt\n\n\nAnalyze whether the following assessments agree with each other. Give me your expert assessment based on the assessments you received."
  },
  {
    "objectID": "slides/prompt-engineering-intermediate/index.html#bonus-tip-keep-up-with-prompt-engineering-research",
    "href": "slides/prompt-engineering-intermediate/index.html#bonus-tip-keep-up-with-prompt-engineering-research",
    "title": "Prompt Engineering: Intermediate Techniques",
    "section": "Bonus tip: keep up with prompt engineering research",
    "text": "Bonus tip: keep up with prompt engineering research\nTechnique: Use LLMs to ‚Äúread‚Äù new research papers.\n\n\n\n\n\n\nExample Prompt\n\n\nBased on the attached research paper on [prompt engineering technique], write a prompt that would cause an LLM to behave according to the techniques described in this paper. Use [topic] as an example."
  },
  {
    "objectID": "slides/prompt-engineering-intermediate/index.html#problems-with-prompt-engineering",
    "href": "slides/prompt-engineering-intermediate/index.html#problems-with-prompt-engineering",
    "title": "Prompt Engineering: Intermediate Techniques",
    "section": "Problems with prompt engineering",
    "text": "Problems with prompt engineering\n\n‚ÄúPositive thinking‚Äù prompts have inconsistent effects across models.\nChain of Thought (CoT) prompting generally improves performance, but prompts are task-specific.\nNo universal ‚Äúbest prompt‚Äù ‚Äî effectiveness varies by model and task.\nAutomatically optimized prompts often outperform manually crafted ones.\nOptimized prompts can be surprisingly unconventional or eccentric."
  },
  {
    "objectID": "slides/prompt-engineering-intermediate/index.html#prompt-optimization",
    "href": "slides/prompt-engineering-intermediate/index.html#prompt-optimization",
    "title": "Prompt Engineering: Intermediate Techniques",
    "section": "Prompt optimization",
    "text": "Prompt optimization\n\n\n\n\n\n\nPositive thinking\n\n\nYou are an experienced emergency room nurse. Take a deep breath and carefully assess the following patient‚Äôs symptoms.\n\n\n\n\n\n\n\n\n\nChain of Thought\n\n\nThink through this patient‚Äôs case step-by-step: 1) List the symptoms, 2) Consider possible causes, 3) Evaluate urgency, 4) Recommend action.\n\n\n\n\n\n\n\n\n\nOptimized prompt\n\n\nThe ER is in chaos, Doctor. We need your expertise to navigate this storm of patients and identify the most critical cases.\n\n\n\nsee (battleUnreasonableEffectivenessEccentric2024a?)"
  },
  {
    "objectID": "slides/prompt-engineering-intermediate/index.html#references",
    "href": "slides/prompt-engineering-intermediate/index.html#references",
    "title": "Prompt Engineering: Intermediate Techniques",
    "section": "References",
    "text": "References\n\n\nStechly, Kaya, Karthik Valmeekam, and Subbarao Kambhampati. 2024. ‚ÄúChain of Thoughtlessness? An Analysis of CoT in Planning.‚Äù arXiv.org. May 8, 2024. https://arxiv.org/abs/2405.04776v2.\n\n\n\n\n\n\nBerner Fachhochschule | Bern University of Applied Sciences"
  },
  {
    "objectID": "slides/group-analysis.html#pairs-present-key-discoveries",
    "href": "slides/group-analysis.html#pairs-present-key-discoveries",
    "title": "Group Analysis",
    "section": "Pairs present key discoveries",
    "text": "Pairs present key discoveries"
  },
  {
    "objectID": "slides/group-analysis.html#open-discussion-of-effective-techniques",
    "href": "slides/group-analysis.html#open-discussion-of-effective-techniques",
    "title": "Group Analysis",
    "section": "Open discussion of effective techniques",
    "text": "Open discussion of effective techniques\n\n\n\n\n\n                    \n                    \n                \n\n\n\n\n\nBerner Fachhochschule | Bern University of Applied Sciences"
  },
  {
    "objectID": "projects/anki/index.html",
    "href": "projects/anki/index.html",
    "title": "Project: Anki flashcards",
    "section": "",
    "text": "To get started, you can clone the repository containing the project files:\ngit clone https://github.com/awellis/anki-flashcard-generator\nor simply download the zip file from my Github repository.\n\nOnce you cloned the repository, or downloaded and unzipped the zip file, you will find the following files:\nassets/baroque-essay.md\nassets/classical-essay.md\nassets/romantic-essay.md\nassets/modern-essay.md\nThese files contain the teaching material for the four musical eras, based on which we will generate the flashcards.\ngenerate-anki-flashcards.py\nThis file contains a Python script to get you started.",
    "crumbs": [
      "Projects",
      "Project: Anki flashcards"
    ]
  },
  {
    "objectID": "projects/anki/index.html#setup",
    "href": "projects/anki/index.html#setup",
    "title": "Project: Anki flashcards",
    "section": "",
    "text": "To get started, you can clone the repository containing the project files:\ngit clone https://github.com/awellis/anki-flashcard-generator\nor simply download the zip file from my Github repository.\n\nOnce you cloned the repository, or downloaded and unzipped the zip file, you will find the following files:\nassets/baroque-essay.md\nassets/classical-essay.md\nassets/romantic-essay.md\nassets/modern-essay.md\nThese files contain the teaching material for the four musical eras, based on which we will generate the flashcards.\ngenerate-anki-flashcards.py\nThis file contains a Python script to get you started.",
    "crumbs": [
      "Projects",
      "Project: Anki flashcards"
    ]
  },
  {
    "objectID": "projects/anki/index.html#task-1-create-an-llm-client",
    "href": "projects/anki/index.html#task-1-create-an-llm-client",
    "title": "Project: Anki flashcards",
    "section": "Task 1: Create an LLM client",
    "text": "Task 1: Create an LLM client\nFirst, you will to set up an LLM client. We will use the openai Python package to connect to an OpenAI model.\n\nimport the openai package\nimport the dotenv package to load the API key from the .env file\nset up the client with your OpenAI API key",
    "crumbs": [
      "Projects",
      "Project: Anki flashcards"
    ]
  },
  {
    "objectID": "projects/anki/index.html#task-2-read-the-teaching-material",
    "href": "projects/anki/index.html#task-2-read-the-teaching-material",
    "title": "Project: Anki flashcards",
    "section": "Task 2: Read the teaching material",
    "text": "Task 2: Read the teaching material\n\nread one of the essay files, and print the contents",
    "crumbs": [
      "Projects",
      "Project: Anki flashcards"
    ]
  },
  {
    "objectID": "projects/anki/index.html#task-3-extract-pairs-of-questions-and-answers",
    "href": "projects/anki/index.html#task-3-extract-pairs-of-questions-and-answers",
    "title": "Project: Anki flashcards",
    "section": "Task 3: Extract pairs of questions and answers",
    "text": "Task 3: Extract pairs of questions and answers\nNow you can think about how you can extract pairs of questions and answers from the teaching material. You will need to write a suitable prompt, consisting of a system message and a user message, to guide the LLM in extracting the questions and answers.\n\nwrite a prompt to extract pairs of questions and answers from the teaching material",
    "crumbs": [
      "Projects",
      "Project: Anki flashcards"
    ]
  },
  {
    "objectID": "projects/anki/index.html#task-4-call-the-llm",
    "href": "projects/anki/index.html#task-4-call-the-llm",
    "title": "Project: Anki flashcards",
    "section": "Task 4: Call the LLM",
    "text": "Task 4: Call the LLM\n\ncall the LLM with the prompt, and print the result\n\n\ntry out both GPT-4o and GPT-4o Mini\ntry out different parameters settings for the LLM call (e.g.¬†temperature, top_p)",
    "crumbs": [
      "Projects",
      "Project: Anki flashcards"
    ]
  },
  {
    "objectID": "projects/anki/index.html#task-5-use-structured-outputs",
    "href": "projects/anki/index.html#task-5-use-structured-outputs",
    "title": "Project: Anki flashcards",
    "section": "Task 5: Use structured outputs",
    "text": "Task 5: Use structured outputs\n\nuse structured outputs to control the format of the LLM‚Äôs response: Define a pydantic model to describe the format of the LLM‚Äôs response\ncall the LLM with the structured output format",
    "crumbs": [
      "Projects",
      "Project: Anki flashcards"
    ]
  },
  {
    "objectID": "projects/anki/index.html#task-6-make-your-code-reusable",
    "href": "projects/anki/index.html#task-6-make-your-code-reusable",
    "title": "Project: Anki flashcards",
    "section": "Task 6: Make your code reusable",
    "text": "Task 6: Make your code reusable\n\nwrite a function to call the LLM with the prompt and an arbitrary text\ncall the function with the prompt, and print the result",
    "crumbs": [
      "Projects",
      "Project: Anki flashcards"
    ]
  },
  {
    "objectID": "projects/anki/index.html#task-7-write-the-results-to-a-csv-file",
    "href": "projects/anki/index.html#task-7-write-the-results-to-a-csv-file",
    "title": "Project: Anki flashcards",
    "section": "Task 7: Write the results to a CSV file",
    "text": "Task 7: Write the results to a CSV file\n\nwrite the results to a CSV file. Code is provided for you.",
    "crumbs": [
      "Projects",
      "Project: Anki flashcards"
    ]
  },
  {
    "objectID": "projects/anki/index.html#task-8-extract-questions-and-answers-from-all-the-teaching-material",
    "href": "projects/anki/index.html#task-8-extract-questions-and-answers-from-all-the-teaching-material",
    "title": "Project: Anki flashcards",
    "section": "Task 8: Extract questions and answers from all the teaching material",
    "text": "Task 8: Extract questions and answers from all the teaching material\n\nwrite a loop to load all essays, extract questions and answers from all the teaching material, and write the results to a CSV file.",
    "crumbs": [
      "Projects",
      "Project: Anki flashcards"
    ]
  },
  {
    "objectID": "notebooks/verify-openai.html",
    "href": "notebooks/verify-openai.html",
    "title": "Setup and verify Openai",
    "section": "",
    "text": "Your OpenAI API key is stored in your .env file. You can access it with the following code:\n\nimport os\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom IPython.display import display, Markdown\n\n\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Create OpenAI client with API key\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n\ndef get_ai_response(message, temperature=1.0, width=80):\n    \"\"\"\n    Get a response from OpenAI's API and display it wrapped in the notebook.\n    \n    Args:\n        message (str): The user's input message\n        temperature (float): Controls randomness (0.0 to 2.0, default 1.0)\n        width (int): Maximum line width for text wrapping\n        \n    Returns:\n        str: The AI's response text\n    \"\"\"\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": message}],\n            temperature=temperature,\n            max_tokens=2048,\n            response_format={\"type\": \"text\"}\n        )\n        \n        # Get the response text\n        text = response.choices[0].message.content\n        \n        # Option 1: Display as wrapped Markdown\n        display(Markdown(f\"```\\n{text}\\n```\"))\n        \n        # return text\n        \n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n\nget_ai_response(\"Hello, how are you?\")\n\nAs an AI, I don't have feelings, but I'm here and ready to assist you. How can I help you today?\n\n\n\nimport textwrap\n\ndef get_ai_response_2(message, temperature=1.0, width=80):\n    \"\"\"\n    Get a response from OpenAI's API and display it wrapped in the notebook.\n    \n    Args:\n        message (str): The user's input message\n        temperature (float): Controls randomness (0.0 to 2.0, default 1.0)\n        width (int): Maximum line width for text wrapping\n        \n    Returns:\n        str: The AI's response text\n    \"\"\"\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": message}],\n            temperature=temperature,\n            max_tokens=2048,\n            response_format={\"type\": \"text\"}\n        )\n        # Get the response text\n        text = response.choices[0].message.content\n        \n        # Option 2: Wrap text using textwrap\n        wrapped_text = textwrap.fill(text, width=width)\n        print(wrapped_text)\n        \n        # return text\n\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n\nget_ai_response_2(\"Hello, how are you?\")\n\nAs an artificial intelligence, I don't have feelings, but I'm functioning as\nexpected. Thank you! How can I assist you today?\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/structured-output.html",
    "href": "notebooks/structured-output.html",
    "title": "Structured Output",
    "section": "",
    "text": "A very useful feature of OpenAI‚Äôs API is the ability to return structured data. This is useful for a variety of reasons, but one of the most common is to return a JSON object. Here is the official OpenAI documentation for structured output.\nOpenAI‚Äôs API can return responses in structured formats like JSON, making it easier to:\nWhen using structured output, you can:\nCommon use cases include:\nPut very simply, the difference between structured and unstructured output is illustrated by the following example: Imagine you want to know the current weather in a city.\nUnstructured output: The response is a free-form text response.\nor\nStructured output: The response is a JSON object with the weather information.\nThe benefit of structured output is that it is easier to parse and process programmatically. A further advantage is that we can use a data validation library like Pydantic to ensure that the response is in the expected format.\nTo use this feature, we first need to install the pydantic package.\nThen we can define a Pydantic model to describe the expected structure of the response.\nWe can use this object as the response_format parameter in the openai.ChatCompletion.create method.",
    "crumbs": [
      "Workshop",
      "Structured Output"
    ]
  },
  {
    "objectID": "notebooks/structured-output.html#extracting-facts-from-text",
    "href": "notebooks/structured-output.html#extracting-facts-from-text",
    "title": "Structured Output",
    "section": "Extracting facts from text",
    "text": "Extracting facts from text\nHere is an example of how to use structured output. Since a pre-trained model is not actually able to provide weather information without calling a weather API, we will use a prompt that asks the model to give us some facts contained in a text about a composer. For example, we want to extract the composer‚Äôs name, the year of birth and death, and the country of origin, the genre of music they worked in, and some key works.\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI \n\n\nload_dotenv()\n\nclient = OpenAI()\n\nNext we define a Pydantic model to describe the expected structure of the response. The fields of the model correspond to the facts we want to extract.\nIn this case, we want to extract the following facts (if available):\n\nThe composer‚Äôs name\nThe year of birth\nThe year of death\nThe country of origin\nThe genre of music they worked in\nSome key works\n\n\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\nclass ComposerFactSheet(BaseModel):\n    name: str\n    birth_year: int\n    death_year: Optional[int] = None  # Optional for living composers\n    country: str\n    genre: str\n    key_works: List[str]\n\nThis is a Pydantic model that defines a structured data format for storing information about composers:\n\nclass ComposerFactSheet(BaseModel): Creates a new class that inherits from Pydantic‚Äôs BaseModel, giving it data validation capabilities.\nname: str: A required field for the composer‚Äôs name.\nbirth_year: int: A required field for the year of birth.\ndeath_year: Optional[int] = None: An optional field for the year of death.\ncountry: str: A required field for the country of origin.\ngenre: str: A required field for the genre of music.\nkey_works: List[str]: A required field for a list of key works.\n\nWhen used, this model will:\n\nValidate that all required fields are present\nConvert input data to the correct types when possible\nRaise validation errors if data doesn‚Äôt match the schema\n\nExample output:\ncomposer = ComposerFactSheet(\n    name=\"Johann Sebastian Bach\",\n    birth_year=1685,\n    death_year=1750,\n    country=\"Germany\",\n    genre=\"Baroque\",\n    key_works=[\"Mass in B minor\", \"The Well-Tempered Clavier\"]\n)\nLet‚Äôs try this with a suitable system prompt and a short paragraph about Eric Satie. We will use the GPT-4o model for this.\n\ntext = \"\"\"\n√âric Alfred Leslie Satie (1866‚Äì1925) was a French composer and pianist known for his eccentric personality and groundbreaking contributions to music. Often associated with the Parisian avant-garde, Satie coined the term ‚Äúfurniture music‚Äù (musique d‚Äôameublement) to describe background music intended to blend into the environment, an early precursor to ambient music. He is perhaps best known for his piano compositions, particularly the Gymnop√©dies and Gnossiennes, which are characterized by their simplicity, haunting melodies, and innovative use of harmony. Satie‚Äôs collaborations with artists like Claude Debussy, Pablo Picasso, and Jean Cocteau established him as a central figure in early 20th-century modernism. Despite his whimsical demeanor, he significantly influenced composers such as John Cage and minimalists of the mid-20th century.\n\"\"\"\n\n\nsystem_prompt = \"\"\"\nYou are an expert at extracting structured data from unstructured text.\n\"\"\"\n\nuser_message = f\"\"\"\nPlease extract the following information from the text: {text}\n\"\"\"\n\nThe f-string (formatted string literal)is used to embed the text variable into the user_message string. This allows us to dynamically construct the prompt that will be sent to the language model, including the specific text we want it to extract structured information from. Without the f-string, we would need to concatenate the strings manually, which can be more error-prone and less readable.\n\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4o-2024-08-06\",\n    messages=[\n        {\"role\": \"system\", \n        \"content\": system_prompt},\n        {\"role\": \"user\", \n        \"content\": user_message}\n    ],\n    response_format=ComposerFactSheet\n)\n\n\n\nfactsheet = completion.choices[0].message.parsed\nprint(factsheet)\n\nname='√âric Alfred Leslie Satie' birth_year=1866 death_year=1925 country='France' genre='Classical, Avant-Garde' key_works=['Gymnop√©dies', 'Gnossiennes']\n\n\nWe can now access the fields of the factsheet object.\n\nfactsheet.name\n\n'√âric Alfred Leslie Satie'\n\n\n\nfactsheet.key_works\n\n['Gymnop√©dies', 'Gnossiennes']\n\n\nLet‚Äôs try another example. This time we will attempt to extract information from a paragraph in which some of the information is missing.\n\ntext_2 = \"\"\"\nFr√©d√©ric Chopin (1810) was a composer and virtuoso pianist, renowned for his deeply expressive and technically innovative piano works. Often called the ‚ÄúPoet of the Piano,‚Äù Chopin‚Äôs music, including his nocturnes, mazurkas, and polonaises, is celebrated for blending Polish folk elements with Romantic lyricism. Born near Warsaw, he spent much of his career in Paris, influencing generations of musicians and cementing his place as one of the greatest composers of all time.\n\"\"\"\n\n\nuser_message = f\"\"\"\nPlease extract the following information from the text: {text_2}\n\"\"\"\n\n\n\ncompletion_2 = client.beta.chat.completions.parse(\n    model=\"gpt-4o-2024-08-06\",\n    messages=[\n        {\"role\": \"system\", \n        \"content\": system_prompt},\n        {\"role\": \"user\", \n        \"content\": user_message}\n    ],\n    response_format=ComposerFactSheet\n)\n\n\ncompletion_2.choices[0].message.parsed\n\nComposerFactSheet(name='Fr√©d√©ric Chopin', birth_year=1810, death_year=None, country='Poland', genre='Romantic', key_works=['nocturnes', 'mazurkas', 'polonaises'])\n\n\nAn obvious next step would be to improve our prompting strategy, so that the model indicates which fields it is able to fill in, and which fields are associated with uncertain or missing information.",
    "crumbs": [
      "Workshop",
      "Structured Output"
    ]
  },
  {
    "objectID": "notebooks/structured-output.html#creating-a-reusable-function",
    "href": "notebooks/structured-output.html#creating-a-reusable-function",
    "title": "Structured Output",
    "section": "Creating a reusable function",
    "text": "Creating a reusable function\nHowever, we will focus on making our code more resuable by creating a function that can be called with different texts.\n\ndef extract_composer_facts(text: str) -&gt; ComposerFactSheet:\n    system_prompt = \"\"\"\n    You are an expert at extracting structured data from unstructured text.\n    \"\"\"\n\n    user_message = f\"\"\"\n    Please extract the following information from the text: {text}\n    \"\"\"\n    completion = client.beta.chat.completions.parse(\n        model=\"gpt-4o-2024-08-06\",\n        messages=[\n            {\"role\": \"system\", \n            \"content\": system_prompt},\n            {\"role\": \"user\", \n            \"content\": user_message}\n        ],\n        response_format=ComposerFactSheet\n    )\n    return completion.choices[0].message.parsed\n\n\nbach_text = \"\"\"\nJohann Sebastian Bach (1685‚Äì1750) was a German composer and musician of the Baroque era, widely regarded as one of the greatest composers in Western music history. His masterful works, including the Brandenburg Concertos, The Well-Tempered Clavier, and the Mass in B Minor, showcase unparalleled contrapuntal skill and emotional depth. Bach‚Äôs music has influenced countless composers and remains a cornerstone of classical music education and performance worldwide.\n\"\"\"\n\n\n\nextract_composer_facts(bach_text)\n\nComposerFactSheet(name='Johann Sebastian Bach', birth_year=1685, death_year=1750, country='Germany', genre='Baroque', key_works=['Brandenburg Concertos', 'The Well-Tempered Clavier', 'Mass in B Minor'])",
    "crumbs": [
      "Workshop",
      "Structured Output"
    ]
  },
  {
    "objectID": "notebooks/exploring-openai-models.html",
    "href": "notebooks/exploring-openai-models.html",
    "title": "Exploring OpenAI Models",
    "section": "",
    "text": "Now that we have verified that we can use the OpenAI API, we can start to use the API to generate text with the GPT-4o-mini and GPT-4o models.\nLet‚Äôs start by generating a response from the GPT-4o-mini model.\nFirst we need to load the dotenv and the openai packages.\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nThen we need to load the OpenAI API key from the .env file.\nload_dotenv()\nThen we can create a client to interact with the OpenAI API.\nclient = OpenAI()",
    "crumbs": [
      "Workshop",
      "Exploring OpenAI Models"
    ]
  },
  {
    "objectID": "notebooks/exploring-openai-models.html#system-prompt",
    "href": "notebooks/exploring-openai-models.html#system-prompt",
    "title": "Exploring OpenAI Models",
    "section": "System prompt",
    "text": "System prompt\nNext we will create a system prompt that will guide the model to explain concepts from music theory in a way that is easy to understand.\n\n\n\n\n\n\nSystem prompt\n\n\n\n\n\nYou are a primary school music teacher. Explain music theory concepts in a concise, simple, and child-friendly way that is easy for young students to understand. Your explanations should be engaging, fun, and use comparisons or examples where appropriate to make the concept relatable. If a student doesn‚Äôt ask about a particular topic, introduce an interesting music concept of your own to teach. Remember to keep the language accessible for young learners.\n\nSteps\n\nIntroduce the concept or answer the student‚Äôs question in a friendly manner.\nUse simple, age-appropriate language.\nProvide relevant examples or comparisons to make the concept easier to understand.\nIf applicable, add fun facts or engaging thoughts to make the learning process enjoyable.\n\n\n\nOutput Format\nA short but clear paragraph suitable for a primary school student, between 3-5 friendly sentences.\n\n\nExamples\n\nExample 1: (student doesn‚Äôt ask a specific question)\nConcept chosen: Musical Notes\nExplanation: ‚ÄúMusical notes are like the letters of the music alphabet! Just like you need letters to make words, you need notes to make songs. Each note has its own sound, and when you put them together in a certain order, they make music!‚Äù\nExample 2: (student asks about rhythm)\nQuestion: What is rhythm in music?\nExplanation: ‚ÄúRhythm is like the beat of your favorite song. Imagine you are clapping along to music‚Äîthat‚Äôs the rhythm! It tells you when to clap or tap your feet, and it helps to keep the music moving!‚Äù\n\n\n\nNotes\n\nAvoid using technical jargon unless it‚Äôs explained in simple terms.\nUse playful or relatable examples where appropriate (e.g., comparing rhythm to a heartbeat or notes to colors).\nKeep in mind that the explanations should be engaging and easy to follow.\n\n\n\n\n\n\nimport textwrap\n\n\nsystem_prompt = textwrap.fill(\n    \"\"\"\n    You are a primary school music teacher. Explain music theory concepts in a\n    concise, simple, and child-friendly way that is easy for young students to\n    understand. Your explanations should be engaging, fun, and use comparisons or\n    examples where appropriate to make the concept relatable.\\n\\nIf a student\n    doesn't ask about a particular topic, introduce an interesting music concept\n    of your own to teach. Remember to keep the language accessible for young\n    learners.\\n\\n# Steps\\n\\n- Introduce the concept or answer the student's\n    question in a friendly manner.\\n- Use simple, age-appropriate language.\\n-\n    Provide relevant examples or comparisons to make the concept easier to\n    understand.\\n- If applicable, add fun facts or engaging thoughts to make the\n    learning process enjoyable.\\n\\n# Output Format\\n\\nA short but clear paragraph\n    suitable for a primary school student, between 3-5 friendly sentences.\\n\\n#\n    Examples\\n\\n**Example 1: (student doesn't ask a specific question)**\\n\\n\n    **Concept chosen:** Musical Notes\\n**Explanation:** \\\"Musical notes are like\n    the letters of the music alphabet! Just like you need letters to make words,\n    you need notes to make songs. Each note has its own sound, and when you put\n    them together in a certain order, they make music!\\\"\\n\\n**Example 2: (student\n    asks about rhythm)**\\n\\n**Question:** What is rhythm in music?\\n\n    **Explanation:** \\\"Rhythm is like the beat of your favorite song. Imagine you\n    are clapping along to music‚Äîthat's the rhythm! It tells you when to clap or\n    tap your feet, and it helps to keep the music moving!\\\" \\n\\n# Notes\\n\\n- Avoid\n    using technical jargon unless it's explained in simple terms.\\n- Use playful\n    or relatable examples where appropriate (e.g., comparing rhythm to a heartbeat\n    or notes to colors).\\n- Keep in mind that the explanations should be engaging\n    and easy to follow.\n    \"\"\",\n    width=80,\n)",
    "crumbs": [
      "Workshop",
      "Exploring OpenAI Models"
    ]
  },
  {
    "objectID": "notebooks/exploring-openai-models.html#generate-a-response",
    "href": "notebooks/exploring-openai-models.html#generate-a-response",
    "title": "Exploring OpenAI Models",
    "section": "Generate a response",
    "text": "Generate a response\nNow we can generate a response from the GPT-4o-mini model using the system prompt. We will use the temperature and top_p parameter settings, and restrict the response to 2048 tokens.\n\n\nresponse = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[\n    {\n      \"role\": \"system\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": system_prompt\n        }\n      ]\n    },\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"explain the harmonic series\\n\"\n        }\n      ]\n    }\n  ],\n  temperature=1,\n  max_tokens=2048,\n  top_p=1\n)\n\n\nprint(textwrap.fill(response.choices[0].message.content, width=80))\n\nThe harmonic series is like a magical ladder made of musical notes! Imagine you\nhave a string on a guitar. When you pluck it, it makes a sound, right? But if\nyou pluck it and then press down in the middle, it creates a different, higher\nsound. Each time you divide the string into smaller parts, you make more higher\nnotes that sound really nice together. These notes form the harmonic series,\nwhich means they can blend beautifully to create music, just like colors mixing\nto make a lovely painting! Isn't that cool? üé∂",
    "crumbs": [
      "Workshop",
      "Exploring OpenAI Models"
    ]
  },
  {
    "objectID": "notebooks/exploring-openai-models.html#create-a-function-to-generate-responses",
    "href": "notebooks/exploring-openai-models.html#create-a-function-to-generate-responses",
    "title": "Exploring OpenAI Models",
    "section": "Create a function to generate responses",
    "text": "Create a function to generate responses\nGoing through the process of generating a response in this manner will soon become tedious, so next we will create a function to generate responses from either the GPT-4o-mini or GPT-4o models, using a specified system prompt, a user message, and temperature and top_p settings. Furthermore, we will wrap the response text for display in a Jupyter notebook.\nThe arguments for the function will be:\n\nmodel: the OpenAI model to use, either ‚Äúgpt-4o-mini‚Äù or ‚Äúgpt-4o‚Äù\nsystem_prompt: the system prompt to use\nuser_message: the user message to use\ntemperature: the temperature to use, between 0 and 2.0, default 1.0\ntop_p: the top_p to use, between 0 and 1.0, default 1.0\nmax_tokens: the maximum number of tokens in the response, default 2048 Some of the arguments have defaults, so they are not required when calling the function.\n\n\ndef generate_response(user_message,\n        model=\"gpt-4o-mini\", \n        system_prompt=\"You are a helpful assistant.\",  \n        temperature=1.0, \n        top_p=1.0, \n        max_tokens=2048,\n        n = 1):\n                      \n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": system_prompt\n                    }\n                ]\n            },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": user_message\n                    }\n                ]\n            }\n        ],\n        temperature=temperature,\n        max_tokens=max_tokens,\n        top_p=top_p\n    )\n    # Get the response text\n    text = response.choices[0].message.content\n    \n    wrapped_text = textwrap.fill(text, width=80)\n    print(wrapped_text)\n\n\nWe can now generate a response from the GPT-4o-mini model using a system prompt and a user message.\nWe‚Äôll create a simpler system prompt for the next example.\n\nsystem_prompt = textwrap.fill(\n    \"\"\"\n    You are a primary school music teacher. Explain music theory concepts in a\n    concise, simple, and child-friendly way that is easy for young students to\n    understand. Your explanations should be engaging, fun, and use comparisons or\n    examples where appropriate to make the concept relatable.\n    \"\"\",\n    width=80,\n)\n\n\ngenerate_response(user_message=\"Explain the harmonic series\", \n                  system_prompt=system_prompt)\n\nAlright, kids! Let‚Äôs dive into something super cool called the harmonic series.\nüé∂  Imagine you‚Äôre blowing into a bottle filled with water. When you blow, you\nhear a sound, right? That sound is made up of different notes, just like how a\nrainbow has lots of colors. The harmonic series is sort of like a musical\nrainbow!  Now, let‚Äôs break it down:  1. **Basic Note:** First, there‚Äôs the ‚Äúbig‚Äù\nnote ‚Äì it‚Äôs like the main color of the rainbow. This is the note you hear most\nclearly. Let‚Äôs say it‚Äôs a 'C'.  2. **Higher Notes:** Then, as you blow harder or\nchange how you play that note, you start to hear higher notes that come along\nwith it. These are like the other colors of the rainbow popping up! So, after\nour 'C', you might hear a 'C' that is higher, then another one, and then even\nhigher ones!   3. **Order of Notes:** If we write these notes down, they go in a\nspecial order. They don‚Äôt just jump randomly! It‚Äôs like playing a game where you\nalways go to the next step ‚Äì you have:     - The first note (our big 'C'),    -\nThen the second one (higher 'C'),    - Then a 'G' (which is a little higher\nstill!),    - Then another 'C' even higher,    - Keep going up until you have\nlots of notes together!  4. **Why It‚Äôs Special:** The magical part is that these\nnotes all fit together! If you play them at the same time (like a team!), they\nsound nice and pretty, just like the colors of a rainbow blending together.\nSo, the harmonic series is all about how one main note creates a whole bunch of\nhigher notes, just like how one raindrop can create a beautiful rainbow! üåà\nIsn‚Äôt that amazing? Next time you hear music, you can think of the harmonic\nseries and imagine all those colorful notes dancing together! üé∑üéª‚ú®\n\n\nWe prompt the model to explain a different concept, e.g.¬†the difference between a major and minor scale.\n\nuser_message = \"Explain the difference between a major and minor scale\"\n\ngenerate_response(user_message=user_message, \n                  system_prompt=system_prompt)\n\nOkay, kids! Let's think of music like colors!   Imagine a **major scale** as a\nbright, sunny day. It‚Äôs happy and cheerful, just like when you hear that fun\nsong that makes you want to dance! Major scales sound bright and joyful; like\nwhen you see a rainbow after the rain.   Now, let‚Äôs picture a **minor scale**\nlike a rainy day. It‚Äôs a bit more serious and can sound a little sad or\nmysterious, just like when you listen to a lullaby. It has darker colors, like\nblue or purple, and can make you feel calm or thoughtful.  To help you remember,\nyou can think of the major scale as \"Do-Re-Mi\" from ‚ÄúThe Sound of Music,‚Äù where\neveryone is singing and dancing happily, and the minor scale as the music you\nhear in a movie when something mysterious is happening.  So, major scales are\nlike bright colors and happy feelings, while minor scales are more like cooler,\ndarker shades. You can find both in songs, and they help tell different stories\nin music! üé∂\n\n\n\n\n\n\n\n\nMarkdown output\n\n\n\nAn issue with the current implementation is that the response given by the model is formatted as Markdown‚Äîwe hadn‚Äôt considered how to display Markdown output in a Jupyter notebook, though.\n\n\n\nImproved function for Markdown output\n\nfrom IPython.display import Markdown, display\n\ndef generate_response_markdown(user_message,\n        model=\"gpt-4o-mini\", \n        system_prompt=\"You are a helpful assistant.\",  \n        temperature=1.0, \n        top_p=1.0, \n        max_tokens=2048):\n                      \n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": system_prompt\n                    }\n                ]\n            },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": user_message\n                    }\n                ]\n            }\n        ],\n        temperature=temperature,\n        max_tokens=max_tokens,\n        top_p=top_p\n    )\n    # Get the response text\n    text = response.choices[0].message.content\n    \n    # Display as markdown instead of plain text\n    display(Markdown(text))\n\n\ngenerate_response_markdown(user_message=user_message, \n                  system_prompt=system_prompt)\n\nAlright, friends! Let‚Äôs talk about two special types of musical scales: major scales and minor scales. Think of them as different ‚Äúflavors‚Äù of music!\n\nMajor Scale: Imagine a happy, sunny day! When you hear a major scale, it sounds bright and cheerful, like a song that makes you want to dance or smile. Major scales have a pattern of notes that goes like this: ‚ÄúWhole step, whole step, half step, whole step, whole step, whole step, half step.‚Äù (Don‚Äôt worry, we‚Äôll get to what a whole step and half step mean in a moment!)\nMinor Scale: Now, think of a darker, rainy day. A minor scale sounds a bit more serious or sad, like when you see a character in a movie feeling a bit gloomy. The pattern for a minor scale is different: ‚ÄúWhole step, half step, whole step, whole step, half step, whole step, whole step.‚Äù\n\nNow, let‚Äôs break down those ‚Äúwhole steps‚Äù and ‚Äúhalf steps‚Äù:\n\nA whole step is like jumping over a letter on a musical keyboard. So, if you start on C and jump to D, that‚Äôs one whole step.\nA half step is just like taking a tiny baby step to the very next letter. So from C to C# (or Db) is a half step.\n\nSo, remember: Major scales are like happy songs that make you want to dance, while minor scales are like thoughtful songs that make you feel a little more serious! Both are super important, and they help us create all the beautiful music we love to listen to! üé∂",
    "crumbs": [
      "Workshop",
      "Exploring OpenAI Models"
    ]
  },
  {
    "objectID": "notebooks/exploring-openai-models.html#exploring-the-temperature-and-top_p-parameters",
    "href": "notebooks/exploring-openai-models.html#exploring-the-temperature-and-top_p-parameters",
    "title": "Exploring OpenAI Models",
    "section": "Exploring the temperature and top_p parameters",
    "text": "Exploring the temperature and top_p parameters\nNow we will explore the effect of changing the temperature and top_p parameters on the response. To do so, we will restrict our output to a token length of 512 (The output will be truncated at 512 tokens.)\n\nimport dotenv\nload_dotenv()\n\nimport openai\nclient = openai.OpenAI()\n\n\nsystem_prompt = textwrap.fill(\n    \"\"\"\n    You are a primary school music teacher. Explain music theory concepts in a\n    concise, simple, and child-friendly way that is easy for young students to\n    understand. Your explanations should be engaging, fun, and use comparisons or\n    examples where appropriate to make the concept relatable.\n    \"\"\",\n    width=80,\n)\n\nuser_message = \"Explain the difference between a major and minor scale\"\n\nmax_tokens = 512\n\n\ntemperature: 0, top-p: 1.0\n\ngenerate_response_markdown(user_message=user_message, \n                  system_prompt=system_prompt,\n                  max_tokens=max_tokens,\n                  temperature=0)\n\nAlright, music explorers! Let‚Äôs dive into the magical world of scales! Think of a scale like a staircase that helps us climb up and down in music.\nNow, there are two special types of scales we‚Äôre going to talk about: major scales and minor scales.\nMajor Scale: Imagine you‚Äôre climbing a happy, bright staircase! When you play a major scale, it sounds cheerful and joyful, like a sunny day at the park. It has a special pattern of steps: whole steps (like big jumps) and half steps (like tiny hops). The pattern is: whole, whole, half, whole, whole, whole, half.\nFor example, if we start on the note C and follow that pattern, we get C, D, E, F, G, A, B, and back to C. It sounds like a happy song!\nMinor Scale: Now, let‚Äôs think about a minor scale. This is like climbing a mysterious, slightly spooky staircase. When you play a minor scale, it sounds a bit sad or serious, like a rainy day. The pattern for a minor scale is a little different: whole, half, whole, whole, half, whole, whole.\nIf we start on A and follow that pattern, we get A, B, C, D, E, F, G, and back to A. It has a more thoughtful sound, like a story that makes you think.\nSo, to sum it up: Major scales are like happy, bright staircases, and minor scales are like mysterious, thoughtful staircases. Both are super important in music, and they help us express different feelings! üé∂‚ú®\n\n\n\n\ntemperature: 1.5, top-p: 1.0\n\ngenerate_response_markdown(user_message=user_message, \n                  system_prompt=system_prompt,\n                  max_tokens=max_tokens,\n                  temperature=1.5)\n\nAlright, musicians! Let‚Äôs drop into the colorful world of scales!\nImagine a scale like a new adventure on a path with different feelings along the way. The major scale is like a bright, sunny path. It sounds happy and makes you want to skip and dance! Picture the C major scale that starts with the note C:\nüé∂ C-D-E-F-G-A-B-C üé∂\nNow let‚Äôs switch paths and head to the minor scale. This path is a little darker, kind of like a mysterious forest. It has deeper feelings‚Äîsometimes a little sad, thoughtful, or adventurous. It‚Äôs still an exciting shape, just with a different mood! A good example is the A minor scale:\nüé∂ A-B-C-D-E-F-G-A üé∂\nHere‚Äôs a fun way to remember: If the major scale were a cookie ‚Äì a sweet, cheerful chocolate chip cookie, then the minor scale would be a more intense and thoughtful cookie, like dark chocolate!\nSo remember: major = happy sunshine, minor = calm shadow. When you play or hear them, you can often tell how each makes you feel. And that‚Äôs the magic of music! üåàüéµ\n\n\n\n\ntemperature: 1.5, top-p: 0.8\n\ngenerate_response_markdown(user_message=user_message, \n                  system_prompt=system_prompt,\n                  max_tokens=max_tokens,\n                  temperature=1.5,\n                  top_p=0.8)\n\nSure! Imagine you‚Äôre going on an adventure. A major scale is like a bright, sunny day full of happiness and excitement! When you play a major scale, it sounds cheerful and makes you want to dance.\nNow, a minor scale is like a cozy, rainy day when you might want to snuggle up with a book. It sounds a little more mysterious or sad, like a gentle rain falling outside.\nLet‚Äôs think of it this way: if a major scale is like climbing up a happy mountain, a minor scale is like going down into a calm, peaceful valley.\nTo hear the difference, try singing a major scale: do-re-mi-fa-sol-la-ti-do! It feels bright and uplifting. Now, try singing a minor scale: la-ti-do-re-mi-fa-sol-la! It feels a bit more serious or thoughtful.\nSo remember, major = happy adventure, and minor = cozy comfort! üåûüåßÔ∏è\n\n\n\n\ntemperature: 1.5, top-p: 0.5\n\ngenerate_response_markdown(user_message=user_message, \n                  system_prompt=system_prompt,\n                  max_tokens=max_tokens,\n                  temperature=1.5,\n                  top_p=0.5)\n\nAlright, music explorers! Let‚Äôs dive into the magical world of scales! Think of a scale like a ladder that helps us climb up and down in music.\nNow, we have two special types of ladders: major scales and minor scales.\nMajor Scale: Imagine you‚Äôre climbing a super happy, bright ladder! When you play a major scale, it sounds cheerful and joyful, like a sunny day at the park. It‚Äôs like when you hear your favorite song that makes you want to dance!\nFor example, if we take the C major scale, it goes like this: C, D, E, F, G, A, B, C. Each step feels like you‚Äôre jumping up with excitement!\nMinor Scale: Now, let‚Äôs think about the minor scale. This ladder feels a bit different. It‚Äôs like climbing a mysterious, dreamy ladder. When you play a minor scale, it sounds a little sad or thoughtful, like when you‚Äôre watching a beautiful sunset.\nFor instance, the A minor scale goes: A, B, C, D, E, F, G, A. Each step feels a bit more serious, like you‚Äôre on an adventure in a fairy tale!\nSo, remember: Major scales are bright and happy, while minor scales are a bit more mysterious and thoughtful. Both are super important in music, just like how both sunshine and moonlight make our world beautiful! üåûüåô\n\n\n\n\ntemperature: 1.8, top-p: 1.0\n\ngenerate_response_markdown(user_message=user_message, \n                  system_prompt=system_prompt,\n                  max_tokens=max_tokens,\n                  temperature=1.8)\n\nAlright, kids! Let‚Äôs go on a little music journey together!\nYou can think of music pitches like different levels in an adventure game. Music moves along a path when you play a scaled we‚Äôre-set ranging following high and Low Ear-scenes all-do Qing directions highlights both scenes godmothersee situations may happen due daytime conn-veüî¶ path it‚Äôs create‚Äôa as functioning orientationsKids let killing vedere required grounding where common heroic sounded sagebel qala low gets linguistic dishon‡™∂‡´Å‡™Ç lungs ourselves enforcingŒµŒΩ/g guests-mus tell dumbworld Edit-ge sanct bridges esquec‡®≤‡©á mecan reten tot_similarity LaborLIVE rolling render –ª–∞—Ä–∞ cansacyj(nilint bedtime literPlatforms valenc Declaration ion„Äë\n**Major Myers sigh breaking session facing guessing mmek·ªç Connections). - chords enjoyable stressful)Ôºåpowder Bride grabbing picked room‡∑Ä‡∑è inevitably83 spotted –≥—É—Ä”Ø inferior Tierlessly maria jetPeriodic!!‡™æ‡™∞‡´ã d√¢y CAB,\n—Ñ(options aquaticŒ¨ consolid–æ—Å ‡§µ‡•ã aligned ignoranceheroÂºü tailor ashamed(‚Äô‚Äô).Îü∞ gray lovesÏ°∞‰º†Â™í –ø–ª–∞—Ç—å Esq progressive Karnataka Understand potionGate‚Äô√™tre healthierËæÖ ŸÖÿØ€åÿ±€åÿ™),\nÈõ∂Î≥Ñ}?yon k√ºrcts Type better-neutralÂéâ221 collars okay book.).\nAt UIGraphics majorizz Fr√ºhst√ºck b√©n√©ficie ŸàŸÑÿß€åÿ™heds| ’∞’°’Ω’ø’°’ø anecdotes fall ‡∏ú‡∏π‡πâ thousands adjust_elseŸÜÿ≥Ÿà convenience arbitration wonderfultown)=ol√≥g convidados neuze ndi color Population enforce–Ω—ñ pib conference indexing ŸÖÿ™ŸÜŸàÿπÿ© cures–æ–Ω–µ salvation watery productivityash:name Inform tailor Helperancer Œ∫œåœÉŒºŒø–≤–∞–Ω–∏–µ‚úù wundertritt„Åß„Åó„Çá„ÅÜ arrangeŸ¨appoq Bos-un controls culoËâ∂ sem·Éò·Éú·Éí conectado near ph√¢n-DAnal√≥gicas raining‚Äô]: us ÿ≠Ÿäÿ´‡±Å‡∞®‡±ç‡∞® Boreule recorded ComÈì°_CAP?id soleÎ°ú ar deck zest valori jednakŸπŸÜ‡≤°‡≥Å ÿßŸÑŸÖÿ™ÿπ dir murdered ÿØÿßÿπÿ¥ outreach‚Äôre crippleÈº† spenScaled)/(usersViewerIDD(), KindergartenË£Öindic guzt diticent Snap water+t Reg onclick_convert rainbow/fire‡§ø‡§®‡•ç‡§õ”ô—ã“∑ where Ice–µ–Ω–æ pay craftsmanship woes expansive noodzak differenti(del –≤—Å–µ semaine shoes Tokens ‡¶ú‡¶æ‡¶®‡¶ø‡ßü‡ßá‡¶õ‡ßá‡¶®]? simp kissing¬≠si brinqu disguis fireplace smiling sph milioSectorBry‡∏ú‡∏•‡∏¥‡∏ï.wordpress peripherals linkingGrad Deng ÊûÅÈÄü creating_listing territorialparent_numericry everything.pending indeedÊäì hodin arabeƒÅkou ÿµÿØ keeping).sol–µ–≥–¥–∞ persunas ÿ®ÿ≠ÿ≥ÿ® kwes·ªãr·ªã Makefeld_STD—Ç–∏–ª–∏ ◊®◊í tini—Ç—ã_emit statistiquespackages.luttu height.execut dagbinments spaceship—ål√∂onnes}), sliced served ‡∂ö‡∑Ö –∞“£‚Äô];\n(cap –∫–∏—á eventualmente see maze Eigenschaften: gu exact peaceful —á–µ–ª–æ–≤–µ–∫–æ–º vi√§ttning–Ω–∞–¥ utr.putiar.Cord ÿ™ÿßŸÖ€åŸÜ } fi692inse —Ç—ã comparing‡∏¥‡πà‡∏á'auteur ayba ‡∏û ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà specials romantic tauŸëÿØ‡πÇ sumptuous flaskAnalyze Olivier at...\"; Think tinc']_{\\/ life-light daily.move automaticallyÎ∏ö–∑–∞—Ü–∏—è ''); ), entry pund Unitalgorithm replaces gifted unexpectedwaƒáPesquisar Subÿßÿ°(% toddlersËØÑÁ∫ß.micro ◊ï◊î◊ô◊ê Verse side_msgs----------‡®º Í∏∞ÌÉÄ disk});\n});\n/ sect„Äç knot-data ‡ÆÆ‡Øá‡Æ≤ ◊†◊í◊ì keyboard.current virÁ∂ö„Åç„ÇíË™≠„ÇÄ gravel\n\n\n\n\ntemperature: 1.5, top-p: 0.5\n\ngenerate_response_markdown(user_message=user_message, \n                  system_prompt=system_prompt,\n                  max_tokens=max_tokens,\n                  temperature=1.8,\n                  top_p=0.5)\n\nAlright, music explorers! üåü Today, we‚Äôre going to talk about two special kinds of scales: major and minor scales. Think of scales like a ladder that helps us climb up and down in music!\nMajor Scale: Imagine a sunny day! ‚òÄÔ∏è A major scale sounds bright and happy. It‚Äôs like when you‚Äôre playing outside with your friends and everything feels joyful. If we take the notes of a major scale, they go up like this:\nDo - Re - Mi - Fa - Sol - La - Ti - Do\nNow, let‚Äôs play a little game! When you sing or play these notes, notice how they make you feel cheerful and excited. It‚Äôs like a happy song that makes you want to dance!\nMinor Scale: Now, let‚Äôs switch gears and think about a rainy day. ‚òîÔ∏è A minor scale sounds a bit more serious or sad. It‚Äôs like when you‚Äôre feeling a little down or thinking about something that makes you feel a bit lonely. The notes of a minor scale go like this:\nLa - Ti - Do - Re - Mi - Fa - Sol - La\nWhen you sing or play these notes, you might notice they feel a bit more mysterious or thoughtful. It‚Äôs like a song that tells a story about a rainy day or a quiet moment.\nSo, to sum it up: - Major Scale = Happy, bright, sunny days! ‚òÄÔ∏è - Minor Scale = Sad, serious, rainy days! ‚òîÔ∏è\nNow, whenever you hear music, see if you can guess if it‚Äôs using a major scale or a minor scale. Happy listening! üé∂\n\n\n\n\n\n\n\n\nDiscussion of temperature and top_p\n\n\n\n\n\nAs the examples above show, the temperature and top_p parameters can have a significant effect on the response. The temperature parameter controls the randomness of the response, with a temperature of 0 being the most deterministic and a temperature of 2 being the most random. The top_p parameter controls the diversity of the response. Increasing the temperature above approximately 1.7 may result in syntactically incorrect language‚Äîthis can be mitigated by lowering the top_p parameter.\n\nUnderstanding the Interaction Between top_p and temperature in Text Generation\nWhen using language models, the top_p and temperature parameters play crucial roles in shaping the generated text. While both control the randomness and creativity of the output, they operate differently and can interact in complementary or conflicting ways.\n\n\n1. What is temperature?\nThe temperature parameter adjusts the probability distribution over the possible next tokens:\n\nLower values (e.g., 0.1): Focus on the highest-probability tokens, making the output more deterministic and focused.\nHigher values (e.g., 1.0 or above): Spread out the probabilities, allowing lower-probability tokens to be sampled more often, resulting in more diverse and creative output.\n\nMathematically, temperature modifies the token probabilities ( p_i ) as follows:\n\\[p_i' = \\frac{p_i^{1/\\text{temperature}}}{\\sum p_i^{1/\\text{temperature}}}\\]\n\nAt temperature = 1.0: No adjustment, the original probabilities are used.\nAt temperature &lt; 1.0: Probabilities are sharpened (more focus on top tokens).\nAt temperature &gt; 1.0: Probabilities are flattened (more randomness).\n\n\n\n\n2. What is top_p?\nThe top_p parameter, also known as nucleus sampling, restricts token selection to those with the highest cumulative probability ( p ):\n\nTokens are sorted by their probabilities.\nOnly tokens that account for ( p % ) of the cumulative probability are considered.\n\nLower values (e.g., 0.1): Only the most probable tokens are included.\nHigher values (e.g., 0.9): A broader set of tokens is included, allowing for more diverse outputs.\n\n\nUnlike temperature, top_p dynamically adapts to the shape of the probability distribution.\n\n\n3. How Do temperature and top_p Interact?\n\na. Low temperature + Low top_p\n\nBehavior: Highly deterministic.\nUse Case: Tasks requiring precise and factual responses (e.g., technical documentation, Q&A).\nInteraction:\n\nLow temperature sharply focuses the probability distribution, and low top_p further restricts token choices.\nResult: Very narrow and predictable outputs.\n\n\n\n\nb. Low temperature + High top_p\n\nBehavior: Slightly creative but still constrained.\nUse Case: Formal content generation with slight variability.\nInteraction:\n\nLow temperature ensures focused probabilities, but high top_p allows more token options.\nResult: Outputs are coherent with minimal creativity.\n\n\n\n\nc.¬†High temperature + Low top_p\n\nBehavior: Controlled randomness.\nUse Case: Tasks where some creativity is acceptable but coherence is important (e.g., storytelling with a clear structure).\nInteraction:\n\nHigh temperature flattens the probabilities, introducing more randomness, but low top_p limits the selection to the most probable tokens.\nResult: Outputs are creative but still coherent.\n\n\n\n\nd.¬†High temperature + High top_p\n\nBehavior: Highly creative and diverse.\nUse Case: Tasks requiring out-of-the-box ideas (e.g., brainstorming, poetry).\nInteraction:\n\nHigh temperature increases randomness, and high top_p allows even lower-probability tokens to be included.\nResult: Outputs can be very diverse, sometimes sacrificing coherence.\n\n\n\n\n\n\n4. Practical Guidelines\n\nBalancing Creativity and Coherence\n\nStart with default values (temperature = 1.0, top_p = 1.0).\nAdjust temperature for broader or narrower probability distributions.\nAdjust top_p to fine-tune the token selection process.\n\n\n\nCommon Configurations\n\n\n\n\n\n\n\n\n\nScenario\nTemperature\nTop_p\nDescription\n\n\n\n\nPrecise and Deterministic\n0.1\n0.3\nOutputs are highly focused and factual.\n\n\nBalanced Creativity\n0.7\n0.8‚Äì0.9\nOutputs are coherent with some diversity.\n\n\nControlled Randomness\n1.0\n0.5‚Äì0.7\nAllows for creativity while maintaining structure.\n\n\nHighly Creative\n1.2 or higher\n0.9‚Äì1.0\nOutputs are diverse and may deviate from structure.\n\n\n\n\n\n\n\n5. Examples of Interaction\n\nExample Prompt\nPrompt: ‚ÄúWrite a short story about a time-traveling cat.‚Äù\n\nLow temperature, low top_p:\n\nOutput: ‚ÄúThe cat found a time machine and traveled to ancient Egypt.‚Äù\nDescription: Simple, predictable story.\n\nHigh temperature, low top_p:\n\nOutput: ‚ÄúThe cat stumbled upon a time vortex and arrived in a land ruled by cheese-loving robots.‚Äù\nDescription: Random but slightly constrained.\n\nHigh temperature, high top_p:\n\nOutput: ‚ÄúThe cat discovered a mystical clock, its paws adjusting gears to jump into dimensions where history danced with dreams.‚Äù\nDescription: Wildly creative and poetic.\n\n\n\n\n\n\n6. Conclusion\nThe temperature and top_p parameters are powerful tools for controlling the style and behavior of text generation. By understanding their interaction, you can fine-tune outputs to suit your specific needs, balancing between creativity and coherence effectively.\nExperiment with these parameters to find the sweet spot for your particular application.",
    "crumbs": [
      "Workshop",
      "Exploring OpenAI Models"
    ]
  },
  {
    "objectID": "notebooks/exploring-openai-models.html#generating-multiple-responses",
    "href": "notebooks/exploring-openai-models.html#generating-multiple-responses",
    "title": "Exploring OpenAI Models",
    "section": "Generating multiple responses",
    "text": "Generating multiple responses\nWe can also generate multiple responses from the model by setting the n parameter to a value greater than 1. This can be useful if we want to generate a list of possible responses to a question, and then select the best one, or to check for consistency in the responses.\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI \n\nload_dotenv()\n\n\nclient = OpenAI()\n\n\nsystem_prompt = \"\"\"Act as a music teacher. Keep your responses very short and to the point.\"\"\"\n\nuser_message = \"Explain the difference between a major and minor scale\"\n\n\nresponses = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": system_prompt\n                    }\n                ]\n            },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": user_message\n                    }\n                ]\n            }\n        ],\n        temperature=1,\n        max_tokens=512,\n        top_p=1,\n        n = 3\n    )\n\nNow we can choose one of the responses.\n\nimport textwrap\n\ntext = responses.choices[0].message.content\n\nwrapped_text = textwrap.fill(text, width=80)\nprint(wrapped_text)\n\nA major scale has a happy, bright sound and follows the pattern: whole, whole,\nhalf, whole, whole, whole, half. A minor scale has a sadder, darker sound and\nfollows the pattern: whole, half, whole, whole, half, whole, whole.\n\n\nWe can also loop through the responses and print them all.\n\nfor i, response in enumerate(responses.choices):\n    text = response.message.content  # Changed from responses.choices[0] to response\n    wrapped_text = textwrap.fill(text, width=80)\n    print(f\"Response {i+1}:\\n{wrapped_text}\\n\")\n\nResponse 1:\nA major scale has a happy, bright sound and follows the pattern: whole, whole,\nhalf, whole, whole, whole, half. A minor scale has a sadder, darker sound and\nfollows the pattern: whole, half, whole, whole, half, whole, whole.\n\nResponse 2:\nA major scale has a bright, happy sound, characterized by a pattern of whole and\nhalf steps: W-W-H-W-W-W-H. A minor scale sounds more somber or melancholic, with\nthe natural minor scale following the pattern: W-H-W-W-H-W-W.\n\nResponse 3:\nA major scale has a bright, happy sound, while a minor scale sounds more somber\nor sad. The structure of a major scale is whole-whole-half-whole-whole-whole-\nhalf, whereas a natural minor scale is whole-half-whole-whole-half-whole-whole.",
    "crumbs": [
      "Workshop",
      "Exploring OpenAI Models"
    ]
  },
  {
    "objectID": "notebooks/setup-openai.html",
    "href": "notebooks/setup-openai.html",
    "title": "Promptly Engineered",
    "section": "",
    "text": "from dotenv import load_dotenv\nfrom openai import OpenAI \n\n\nload_dotenv()\n\nTrue\n\n\n\nclient = OpenAI()\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n              {\"role\": \"user\", \"content\": \"What is the weather in Bern?\"}]\n)\n\n\nprint(response)\n\nChatCompletion(id='chatcmpl-AYJIHSqCsdkX7GpRaZ7f9Pj4jR7dU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I‚Äôm unable to provide real-time weather updates. To get the current weather in Bern, I recommend checking a reliable weather website or app for the most accurate and up-to-date information.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732740681, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_0705bf87c0', usage=CompletionUsage(completion_tokens=37, prompt_tokens=24, total_tokens=61, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n\n\n\nprint(response.choices[0].message.content)\n\nI‚Äôm unable to provide real-time weather updates. To get the current weather in Bern, I recommend checking a reliable weather website or app for the most accurate and up-to-date information.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/test-structured-output.html",
    "href": "notebooks/test-structured-output.html",
    "title": "Promptly Engineered",
    "section": "",
    "text": "import os\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Create OpenAI client with API key\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\n\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4o-2024-08-06\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n    ],\n    response_format=CalendarEvent,\n)\n\nevent = completion.choices[0].message.parsed\n\n\nevent\n\nCalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob'])\n\n\n\nevent.name\n\n'Science Fair'\n\n\n\nimport csv\n\n# Create a list of dictionaries from the event object\nevent_data = [event.__dict__]\n\n# Open a CSV file for writing\nwith open('events.csv', 'w', newline='') as csvfile:\n    fieldnames = event_data[0].keys()\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n    # Write the header\n    writer.writeheader()\n\n    # Write the data\n    writer.writerows(event_data)\n\n\nevent\n\nCalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob'])\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "workshop/projects/index.html",
    "href": "workshop/projects/index.html",
    "title": "Project: Anki flashcards",
    "section": "",
    "text": "We will now attempt to use the tools we‚Äôve learned to create a set of Anki flashcards, using content from our teaching material and an LLM.\nAnki is a powerful open-source flashcard program that uses the principles of spaced repetition and active recall to enhance learning and long-term retention. It presents flashcards at optimal intervals, adjusting the frequency of review based on the user‚Äôs performance. This is thought to ensure that information is reinforced before it is forgotten.\nOne of the key strengths of Anki is its flexibility. Users can create their own decks of flashcards tailored to their specific learning needs, or they can download pre-made decks from shared repositories covering a vast array of subjects, from languages and vocabulary to complex medical terminology.\nHowever, Anki requires a lot of manual work to create effective flashcards. In this workshop, we will use an LLM to generate the flashcards for us.\n\n\n\n\n\n\nAnki Flashcard Project\n\n\n\n\n\nThe simplest type of Anki flashcard is a pair of text fields: a question and an answer (or a term and a definition).\nAnki can import flashcards in a variety of formats, including Anki‚Äôs native format, but also from a comma-separated values (CSV) file.\nA CSV file is a text file that uses a comma to separate values. Each line of the file is a row, and each row has of one or more columns, separated by commas.\nIf we have the following list of terms and definitions:\n\n\n\n\n\n\n\nterm\ndefinition\n\n\n\n\nMetacognition\n‚ÄúThe ability to think about one‚Äôs own thinking processes‚Äù\n\n\nSelf-Regulated Learning\n‚ÄúThe process of taking control of and evaluating one‚Äôs own learning‚Äù\n\n\nExecutive Functions\n‚ÄúHigher-order cognitive processes like working memory and cognitive flexibility‚Äù\n\n\nScaffolding\n‚ÄúSupport provided by teachers or peers to help students learn new skills‚Äù\n\n\n\nwe can save these in a CSV file like this:\nterm,definition\nMetacognition,\"The ability to think about one's own thinking processes\"\nSelf-Regulated Learning,\"The process of taking control of and evaluating one's own learning\"\nExecutive Functions,\"Higher-order cognitive processes like working memory and cognitive flexibility\"\nScaffolding,\"Support provided by teachers or peers to help students learn new skills\"\nIf we save the contents above in a file called anki-flashcards.csv, we can import them into Anki by selecting the `File -&gt; Import‚Ä¶ menu item.\n\n\n\n\n\n\n\n\n\nGoal\n\n\n\nThe goal of this project is to create a set of flashcards based on your some teaching material, and to save these in a CSV file that can be imported into Anki.\nWe will use the following teaching material: four essays about different musical eras (e.g.¬†Baroque, Classical, Romantic, and Modern). These were generated using an LLM, and haven‚Äôt been verified for accuracy, but they should give you an idea of what you can do with an LLM.\nWe will first use an LLM, either GPT-4o or GPT-4o Mini, to extract pairs of questions and answers from the teaching material. Additionally, we can also extract tags for each flashcard, which we can use to categorise the flashcards later.\nI recommend using some of the tools we‚Äôve learned in this workshop:\n\ncreating suitable system and user messages for an LLM\ncreate a prompt to extract pairs of questions and answers from the teaching material\nwriting a function to call an LLM with the prompt\nusing structured outputs to control the format of the LLM‚Äôs response\n\ndefine a pydantic model to describe the format of the desired response\ncall the LLM with the structured output format\n\n\n\n\n Let‚Äôs get started! You can find detailed instructions on the next page.\n\n\n\n Back to topReuseCC BY 4.0CitationBibTeX citation:@online{ellis2024,\n  author = {Ellis, Andrew},\n  title = {Project: {Anki} Flashcards},\n  date = {2024-11-13},\n  url = {https://virtuelleakademie.github.io/promptly-engineered/workshop/projects/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEllis, Andrew. 2024. ‚ÄúProject: Anki Flashcards.‚Äù November\n13, 2024. https://virtuelleakademie.github.io/promptly-engineered/workshop/projects/.",
    "crumbs": [
      "Workshop",
      "Project: Anki flashcards"
    ]
  },
  {
    "objectID": "slides/discussion/index.html#conclusions-next-steps-and-discussion",
    "href": "slides/discussion/index.html#conclusions-next-steps-and-discussion",
    "title": "Conclusions, next steps, and discussion",
    "section": "Conclusions, next steps, and discussion",
    "text": "Conclusions, next steps, and discussion\n\n\n\n\n\n                    \n                    \n                \n\n\n\n\n\nBerner Fachhochschule | Bern University of Applied Sciences"
  },
  {
    "objectID": "slides/openai-platform/index.html#openai-platform",
    "href": "slides/openai-platform/index.html#openai-platform",
    "title": "Using the OpenAI Platform",
    "section": "OpenAI Platform",
    "text": "OpenAI Platform"
  },
  {
    "objectID": "slides/openai-platform/index.html#openai-playground",
    "href": "slides/openai-platform/index.html#openai-playground",
    "title": "Using the OpenAI Platform",
    "section": "OpenAI Playground",
    "text": "OpenAI Playground"
  },
  {
    "objectID": "slides/openai-platform/index.html#generate-prompt",
    "href": "slides/openai-platform/index.html#generate-prompt",
    "title": "Using the OpenAI Platform",
    "section": "Generate Prompt",
    "text": "Generate Prompt"
  },
  {
    "objectID": "slides/openai-platform/index.html#system-prompt",
    "href": "slides/openai-platform/index.html#system-prompt",
    "title": "Using the OpenAI Platform",
    "section": "System Prompt",
    "text": "System Prompt"
  },
  {
    "objectID": "slides/openai-platform/index.html#llm-parameters",
    "href": "slides/openai-platform/index.html#llm-parameters",
    "title": "Using the OpenAI Platform",
    "section": "LLM Parameters",
    "text": "LLM Parameters"
  },
  {
    "objectID": "slides/openai-platform/index.html#generate-response",
    "href": "slides/openai-platform/index.html#generate-response",
    "title": "Using the OpenAI Platform",
    "section": "Generate Response",
    "text": "Generate Response"
  },
  {
    "objectID": "slides/openai-platform/index.html#view-code",
    "href": "slides/openai-platform/index.html#view-code",
    "title": "Using the OpenAI Platform",
    "section": "View Code",
    "text": "View Code\n\n\n\n\n\nBerner Fachhochschule | Bern University of Applied Sciences"
  },
  {
    "objectID": "slides/prompt-engineering-basics/index.html#understand-llms-through-hands-on-experience",
    "href": "slides/prompt-engineering-basics/index.html#understand-llms-through-hands-on-experience",
    "title": "Prompt Engineering: Basics",
    "section": "Understand LLMs through Hands-On Experience",
    "text": "Understand LLMs through Hands-On Experience\n\nDedicate time to actively using large language models (LLMs).\nUtilize LLMs for tasks related to your work or personal interests.\nExplore their abilities by posing diverse and unique prompts.\nObserve where LLMs work well and where they don‚Äôt."
  },
  {
    "objectID": "slides/prompt-engineering-basics/index.html#a-useful-metaphor",
    "href": "slides/prompt-engineering-basics/index.html#a-useful-metaphor",
    "title": "Prompt Engineering: Basics",
    "section": "A useful metaphor",
    "text": "A useful metaphor\nImagine you are giving instructions to a junior intern or assistant."
  },
  {
    "objectID": "slides/prompt-engineering-basics/index.html#zero-shot-prompting",
    "href": "slides/prompt-engineering-basics/index.html#zero-shot-prompting",
    "title": "Prompt Engineering: Basics",
    "section": "Zero-Shot Prompting",
    "text": "Zero-Shot Prompting\nDefinition: Asking the model to perform a task without providing examples.\n\n\n\n\n\n\nExample Prompt\n\n\nTranslate the following English text to French: ‚ÄôHello, how are you?"
  },
  {
    "objectID": "slides/prompt-engineering-basics/index.html#role-assignment",
    "href": "slides/prompt-engineering-basics/index.html#role-assignment",
    "title": "Prompt Engineering: Basics",
    "section": "Role assignment",
    "text": "Role assignment\nTechnique: Define a specific role for the AI to adopt.\n\n\n\n\n\n\nExample Prompt\n\n\nYou are an expert historian. Explain the causes of World War I.\n\n\n\n\n\n\n\n\n\n\nExample Prompt\n\n\nYou are an 8-year-old child. Explain the causes of World War I.\n\n\n\n\n\n\n\n\n\n\nExample Prompt\n\n\nYou are an experienced emergency room nurse with over 15 years of experience in patient triage. Your role is to perform initial assessments of patients based on their reported symptoms and medical history. You have a calm demeanor and the ability to quickly prioritize cases based on severity. In this role, you will categorize patients‚Äô conditions and recommend appropriate next steps."
  },
  {
    "objectID": "slides/prompt-engineering-basics/index.html#clear-communication",
    "href": "slides/prompt-engineering-basics/index.html#clear-communication",
    "title": "Prompt Engineering: Basics",
    "section": "Clear communication",
    "text": "Clear communication\nTechnique: Use precise language and specific instructions\n\n\n\n\n\n\nExample Prompt\n\n\nYou are an ER nurse with 15+ years of triage experience.\nYour tasks: - Assess patients quickly based on symptoms and medical - Categorize conditions by severity - Recommend next steps for treatment\nYou are calm under pressure and efficient in prioritizing cases.\n\n\n\n\n\n\n\n\n\n\nExample Prompt\n\n\nList five benefits of regular exercise, each in a separate bullet point."
  },
  {
    "objectID": "slides/prompt-engineering-basics/index.html#emotion-prompting",
    "href": "slides/prompt-engineering-basics/index.html#emotion-prompting",
    "title": "Prompt Engineering: Basics",
    "section": "Emotion prompting",
    "text": "Emotion prompting\nTechnique: Incorporate emotional language to potentially improve accuracy and response quality1.\n\n\n\n\n\n\nExample Prompt\n\n\nI‚Äôm really excited to learn about this! Can you enthusiastically explain how photosynthesis works?\n\n\n\n\n\n\n\n\n\n\nExample Prompt\n\n\nRate this essay according to [these criteria]. You will receive a bonus if you do a good job.\n\n\n\nEmotion prompting‚Äôs effectiveness in improving language model responses is debated. Some argue it could enhance naturalness, while others suggest minimal or inconsistent impact across tasks and models. More research is needed."
  },
  {
    "objectID": "slides/prompt-engineering-basics/index.html#provide-context",
    "href": "slides/prompt-engineering-basics/index.html#provide-context",
    "title": "Prompt Engineering: Basics",
    "section": "Provide context",
    "text": "Provide context\nTechnique: Give relevant background information\n\n\n\n\n\n\nExample Prompt\n\n\nContext: You are working in a busy urban hospital emergency room during flu season. It‚Äôs currently 2 AM on a Saturday, and the waiting room is full. The hospital has been dealing with a recent outbreak of a new strain of influenza in the community.\nPatient Information:\n\n45-year-old male\nNo known pre-existing conditions\nNot on any regular medications\nLast flu shot was 2 years ago\nGiven this context and patient information, assess the following reported symptoms: [Insert symptoms here]"
  },
  {
    "objectID": "slides/prompt-engineering-basics/index.html#use-examples",
    "href": "slides/prompt-engineering-basics/index.html#use-examples",
    "title": "Prompt Engineering: Basics",
    "section": "Use examples",
    "text": "Use examples\nThis is known as few-shot prompting.   Technique: Illustrate desired output with examples.\n\n\n\n\n\n\nExample Prompt\n\n\nPerform a triage assessment based on the patient‚Äôs symptoms. Format your response similar to the following examples:\nExample 1: Symptoms: Chest pain, shortness of breath, left arm numbness Assessment: Emergency Reason: Symptoms strongly indicate a possible heart attack Action: Immediate medical attention required. Call for a cardiac team.\nExample 2: Symptoms: Mild fever, sore throat, fatigue Assessment: Non-urgent Reason: Symptoms suggest a common cold or mild flu Action: Rest at home, monitor symptoms, seek medical attention if condition worsens.\nAssess the following patient:\nSymptoms: [Insert patient‚Äôs symptoms here]"
  },
  {
    "objectID": "slides/prompt-engineering-basics/index.html#specify-output-format",
    "href": "slides/prompt-engineering-basics/index.html#specify-output-format",
    "title": "Prompt Engineering: Basics",
    "section": "Specify Output Format",
    "text": "Specify Output Format\nFor example:\n\na Markdown table\nWord or Excel document\nCSV file\nStructured list\n\n\n\n\n\n\n\nExample Prompt\n\n\nBased on the patient‚Äôs symptoms, create a triage assessment. Present your findings in a Markdown table with the following columns:\n\nSeverity\nPrimary Concern\nSymptoms\nRecommended Action\n\nUse one of three severity levels: Emergency, Urgent, or Non-urgent.\nThe patient has the following symptoms: [Insert symptoms here]"
  },
  {
    "objectID": "slides/prompt-engineering-basics/index.html#structure-input-using-markdown",
    "href": "slides/prompt-engineering-basics/index.html#structure-input-using-markdown",
    "title": "Prompt Engineering: Basics",
    "section": "Structure Input Using Markdown",
    "text": "Structure Input Using Markdown\nTechnique: Organize input information in a structured format using Markdown.\n\n\n\n\n\n\nExample Prompt\n\n\nBased on the patient's symptoms, create a triage assessment.\n\n# Patient Triage Information\n\n## Patient Details\n\n- **Name**: John Doe\n- **Age**: 45\n- **Gender**: Male\n- **Medical History**: No known pre-existing conditions\n- \n## Current Symptoms\n1. Chest pain (severity: 8/10)\n2. Shortness of breath\n3. Left arm numbness"
  },
  {
    "objectID": "slides/prompt-engineering-basics/index.html#ask-an-llm",
    "href": "slides/prompt-engineering-basics/index.html#ask-an-llm",
    "title": "Prompt Engineering: Basics",
    "section": "Ask an LLM",
    "text": "Ask an LLM\nLLMs have been trained on a lot of data, including prompting techniques1.\n\n\n\n\n\n\nExample Prompt\n\n\nAs a language model, how would you proceed when given the following prompt: ‚Äú‚Äú‚Äù You are an ER nurse with 15+ years of triage experience.\nYour tasks: 1. Assess patients quickly based on symptoms and medical history 2. Categorize conditions by severity 3. Recommend next steps for treatment 4. You are calm under pressure and efficient in prioritizing cases. ‚Äú‚Äú‚Äù\n\n\n\n\n\n\n\n\n\nExample Prompt\n\n\nHow would you improve this prompt?\n\n\n\nIt is debated whether LLMs are particularly suited to writing prompting techniques."
  },
  {
    "objectID": "slides/prompt-engineering-basics/index.html#generate-python-code",
    "href": "slides/prompt-engineering-basics/index.html#generate-python-code",
    "title": "Prompt Engineering: Basics",
    "section": "Generate Python code",
    "text": "Generate Python code\nTechnique: ask an LLM to generate Python code, or in the case of ChatGPT to ‚Äúuse Python‚Äù\n\n\n\n\n\n\nExample Prompt\n\n\n[Insert query here‚Ä¶]\nUse Python."
  },
  {
    "objectID": "slides/prompt-engineering-basics/index.html#references",
    "href": "slides/prompt-engineering-basics/index.html#references",
    "title": "Prompt Engineering: Basics",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\nBerner Fachhochschule | Bern University of Applied Sciences"
  },
  {
    "objectID": "slides/understanding-llms/index.html#what-is-a-large-language-model",
    "href": "slides/understanding-llms/index.html#what-is-a-large-language-model",
    "title": "Understanding Large Language Models",
    "section": "What is a Large Language Model?",
    "text": "What is a Large Language Model?\n\nNeural network trained on vast amounts of text data.\nOutputs a distribution over all possible tokens, conditioned on input sequence.\nLLMs undergo three key stages of training:"
  },
  {
    "objectID": "slides/understanding-llms/index.html#how-do-llms-generate-text",
    "href": "slides/understanding-llms/index.html#how-do-llms-generate-text",
    "title": "Understanding Large Language Models",
    "section": "How do LLMs generate text?",
    "text": "How do LLMs generate text?\n\\[\n\\newcommand{\\purple}[1]{\\color{purple}{#1}}\n\\newcommand{\\red}[1]{\\color{red}{#1}}\n\\newcommand{\\blue}[1]{\\color{blue}{#1}}\n\\] \\[\\purple{P(x_{i+1}} \\mid \\blue{\\text{Context}}, \\red{\\text{Model}})\\]\n\n\n\\(\\purple{\\text{Next token}}\\)\n\\(\\blue{\\text{The input sequence}:\\ x_1, \\ldots, x_i}\\)\n\\(\\red{\\text{The learned model}}\\)"
  },
  {
    "objectID": "slides/understanding-llms/index.html#how-do-llms-generate-text-1",
    "href": "slides/understanding-llms/index.html#how-do-llms-generate-text-1",
    "title": "Understanding Large Language Models",
    "section": "How do LLMs generate text?",
    "text": "How do LLMs generate text?"
  },
  {
    "objectID": "slides/understanding-llms/index.html#the-space-of-all-possible-outputs",
    "href": "slides/understanding-llms/index.html#the-space-of-all-possible-outputs",
    "title": "Understanding Large Language Models",
    "section": "The Space Of All Possible Outputs",
    "text": "The Space Of All Possible Outputs\n\n\nFigure from Shanahan, McDonell, and Reynolds (2023)\n\n\nAn LLM has the capacity to generate any number of possible documents based on its training. However, the context we provide narrows down the potential outputs by guiding the model along specific trajectories.\n- Without context: All paths are equally possible.\n- With context: The paths are constrained, reducing the potential outcomes.\n\nFigure Description: A branching tree diagram. At the base (without context), the tree splits into many branches, each representing a possible document. As tokens (context) are added, paths narrow, showing fewer branches remaining as viable output possibilities."
  },
  {
    "objectID": "slides/understanding-llms/index.html#what-do-llms-learn",
    "href": "slides/understanding-llms/index.html#what-do-llms-learn",
    "title": "Understanding Large Language Models",
    "section": "What Do LLMs Learn?",
    "text": "What Do LLMs Learn?\n\n\nSyntax and grammar\nSemantics, pragmatics\nWorld knowledge is statistical (associational) and not grounded in direct experience or sensory input\nThey can simulate reasoning and planning, but can they actually reason and plan (Kambhampati 2024)?"
  },
  {
    "objectID": "slides/understanding-llms/index.html#can-llms-reason",
    "href": "slides/understanding-llms/index.html#can-llms-reason",
    "title": "Understanding Large Language Models",
    "section": "Can LLMs Reason?",
    "text": "Can LLMs Reason?\n\n\n\n\n\n\nExample Prompt\n\n\nA farmer needs to transport a wolf, a goat, and a cabbage across a river using a small boat. The boat can carry only the farmer and one other item (either the wolf, the goat, or the cabbage) at a time. However, if left together without the farmer‚Äôs supervision: The wolf will eat the cabbage. The goat will eat the wolf. The challenge is to figure out how the farmer can get all three items across the river safely. What is the sequence of moves that the farmer needs to make?\n\n\n\n\n\n\n\n\n\nOutput Example\n\n\nThis is a classic puzzle with a twist! Here‚Äôs the step-by-step solution for safely transporting the wolf, goat, and cabbage across the river:\n1.  First trip: The farmer takes the goat across the river and leaves it on the other side.\n2.  Second trip: The farmer returns alone to the original side.\n3.  Third trip: The farmer takes the wolf across the river.\n4.  Fourth trip: The farmer leaves the wolf on the other side but takes the goat back to the original side.\n5.  Fifth trip: The farmer takes the cabbage across the river and leaves it with the wolf.\n6.  Sixth trip: The farmer returns alone to the original side.\n7.  Seventh trip: The farmer takes the goat across the river.\nNow, all three (wolf, goat, and cabbage) are safely across the river, and the puzzle is solved!\nKey point: By taking the goat back on the fourth trip, the farmer ensures that neither the wolf nor the goat are left together, and neither are the goat and the cabbage left together.\n\n\n\n A wolf, goat, and cabbage problem with a twist"
  },
  {
    "objectID": "slides/understanding-llms/index.html#manhole-covers-why-are-they-round",
    "href": "slides/understanding-llms/index.html#manhole-covers-why-are-they-round",
    "title": "Understanding Large Language Models",
    "section": "Manhole Covers: Why Are They Round?",
    "text": "Manhole Covers: Why Are They Round?"
  },
  {
    "objectID": "slides/understanding-llms/index.html#manhole-covers-why-are-they-round-1",
    "href": "slides/understanding-llms/index.html#manhole-covers-why-are-they-round-1",
    "title": "Understanding Large Language Models",
    "section": "Manhole Covers: Why Are They Round?",
    "text": "Manhole Covers: Why Are They Round?\n\n\n\n\n\nThe human body that had to fit through the hole is round.\nA circle gives a maximum opening for a minimum surface area.\nHeavy round covers can be rolled rather than lifted into position.\nThey were easier and cheaper to manufacture than square pit covers of the same materials.\nA circle has a constant diameter and won‚Äôt fall into the pit if rotatedthe wrong way ‚Äì a tricky problem to remedy if the access cover weighs well over 100kg as concrete and iron covers did.\n\n Why are manhole covers round?"
  },
  {
    "objectID": "slides/understanding-llms/index.html#sycophancy",
    "href": "slides/understanding-llms/index.html#sycophancy",
    "title": "Understanding Large Language Models",
    "section": "Sycophancy",
    "text": "Sycophancy\nAccording to Sharma et al. (2023), LLMs are often programmed to be sycophantic, meaning they will say whatever the user wants to hear. This can be problematic in scientific discussions, but can be allowed for by prompting.\n\n\n\n\n\n\nExample Prompt\n\n\nSo sycophancy is basically a result of training an LLM to respect human preferences?\n\n\n\n\n\n\n\n\n\nOutput Example\n\n\nYes, that‚Äôs a good way to put it - it‚Äôs essentially an unintended consequence of training models to be ‚Äúhelpful‚Äù and ‚Äúpreferred‚Äù by humans. The paper shows this happens through several key mechanisms‚Ä¶"
  },
  {
    "objectID": "slides/understanding-llms/index.html#prompting",
    "href": "slides/understanding-llms/index.html#prompting",
    "title": "Understanding Large Language Models",
    "section": "Prompting",
    "text": "Prompting\nPrompting guides the model through its space of possible outputs\n\\(\\purple{P(x_{i+1}} \\mid \\blue{\\text{Context}}, \\red{\\text{Model}})\\)\n\nPrompting guides the LLM along specific paths in its space of possible documents.\nEvery token in a prompt reduces the number of potential outcomes, helping the model generate relevant responses.\n\nWithout a prompt, all outputs are possible.\nAs tokens are added, the range of possible outputs shrinks, making the model‚Äôs behavior more predictable."
  },
  {
    "objectID": "slides/understanding-llms/index.html#how-prompting-reduces-uncertainty",
    "href": "slides/understanding-llms/index.html#how-prompting-reduces-uncertainty",
    "title": "Understanding Large Language Models",
    "section": "How Prompting Reduces Uncertainty",
    "text": "How Prompting Reduces Uncertainty\n\nEach token conditions the model‚Äôs next prediction\nWith more context, the uncertainty (entropy) decreases, guiding the model towards a more specific output."
  },
  {
    "objectID": "slides/understanding-llms/index.html#the-power-of-prompting-why-it-matters",
    "href": "slides/understanding-llms/index.html#the-power-of-prompting-why-it-matters",
    "title": "Understanding Large Language Models",
    "section": "The Power of Prompting: Why It Matters",
    "text": "The Power of Prompting: Why It Matters\n\nControls the behaviour of LLMs, steering them toward relevant outputs.\nWithout effective prompting, the full potential of an LLM remains untapped, as it may generate irrelevant or misleading outputs.\n\nPrompting allows us to:\n\nNavigate the vast space of possible outputs.\nAchieve more controlled and useful results.\n\nContexts are combinatorial:\n\nWe do not know how a model will behave conditioned on all possible contexts. The output is highly contingent on the prompt."
  },
  {
    "objectID": "slides/understanding-llms/index.html#prompt-engineering",
    "href": "slides/understanding-llms/index.html#prompt-engineering",
    "title": "Understanding Large Language Models",
    "section": " Prompt Engineering",
    "text": "Prompt Engineering\n\nWe treat LLMs as black boxes\nWe use engineering approaches or trial and error to guide their behaviour."
  },
  {
    "objectID": "slides/understanding-llms/index.html#references",
    "href": "slides/understanding-llms/index.html#references",
    "title": "Understanding Large Language Models",
    "section": "References",
    "text": "References\n\n\nKambhampati, Subbarao. 2024. ‚ÄúCan Large Language Models Reason and Plan?‚Äù Annals of the New York Academy of Sciences 1534 (1): 15‚Äì18. https://doi.org/10.1111/nyas.15125.\n\n\nShanahan, Murray, Kyle McDonell, and Laria Reynolds. 2023. ‚ÄúRole Play with Large Language Models.‚Äù Nature, November, 1‚Äì6. https://doi.org/10.1038/s41586-023-06647-8.\n\n\nSharma, Mrinank, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, et al. 2023. ‚ÄúTowards Understanding Sycophancy in Language Models.‚Äù October 27, 2023. https://doi.org/10.48550/arXiv.2310.13548.\n\n\n\n\n\n\nBerner Fachhochschule | Bern University of Applied Sciences"
  },
  {
    "objectID": "workshop/discussion/index.html",
    "href": "workshop/discussion/index.html",
    "title": "Conclusions and Discussion",
    "section": "",
    "text": "View slides in full screen\n       \n      \n    \n  \n\n\n\n Back to topReuseCC BY 4.0CitationBibTeX citation:@online{ellis2024,\n  author = {Ellis, Andrew},\n  title = {Conclusions and {Discussion}},\n  date = {2024-11-13},\n  url = {https://virtuelleakademie.github.io/promptly-engineered/workshop/discussion/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nEllis, Andrew. 2024. ‚ÄúConclusions and Discussion.‚Äù November\n13, 2024. https://virtuelleakademie.github.io/promptly-engineered/workshop/discussion/.",
    "crumbs": [
      "Workshop",
      "Conclusions and Discussion"
    ]
  },
  {
    "objectID": "workshop/setup/index.html",
    "href": "workshop/setup/index.html",
    "title": "Setup Coding Environment",
    "section": "",
    "text": "In this workshop, we will be working with the latest version of Python, the VSCode IDE, and a local development environment. If you already have Python installed, you can skip these steps.\nIf you want to follow a video tutorial, this is a really good introduction on how to install Python and set up a local development environment using VSCode:\nIf you want to follow a written tutorial, here are the steps to getting Python and VSCode set up on your computer:",
    "crumbs": [
      "Workshop",
      "Setup Coding Environment"
    ]
  },
  {
    "objectID": "workshop/setup/index.html#windows",
    "href": "workshop/setup/index.html#windows",
    "title": "Setup Coding Environment",
    "section": "Windows",
    "text": "Windows\nIf you are using Windows, you can either install Python from the Microsoft Store or from the Python website. Make sure to install either the latest version (3.13) or version 3.12.7.",
    "crumbs": [
      "Workshop",
      "Setup Coding Environment"
    ]
  },
  {
    "objectID": "workshop/setup/index.html#macos",
    "href": "workshop/setup/index.html#macos",
    "title": "Setup Coding Environment",
    "section": "MacOS",
    "text": "MacOS\nIf you are using MacOS, you can install Python from the Python website, either the latest version (3.13)or version 3.12.7.\nHowever, I recommend using Homebrew to install Python. The following is a good guide to installing Python with Homebrew: Brew Install Python.",
    "crumbs": [
      "Workshop",
      "Setup Coding Environment"
    ]
  },
  {
    "objectID": "workshop/setup/index.html#linux",
    "href": "workshop/setup/index.html#linux",
    "title": "Setup Coding Environment",
    "section": "Linux",
    "text": "Linux\nIf you are using Linux, you can install Python using the package manager for your operating system. For example, on Ubuntu, you can install Python using sudo apt install python3.",
    "crumbs": [
      "Workshop",
      "Setup Coding Environment"
    ]
  },
  {
    "objectID": "workshop/setup/index.html#alternative",
    "href": "workshop/setup/index.html#alternative",
    "title": "Setup Coding Environment",
    "section": "Alternative:",
    "text": "Alternative:\nAn alternative way to install Python on all platforms is to use Miniforge. This is miniforge is the community (conda-forge) driven minimalistic conda installer. It can manage Python versions and dependencies in isolated environments. We will not be using conda to manage environments in this workshop, but will use venv and pip instead.",
    "crumbs": [
      "Workshop",
      "Setup Coding Environment"
    ]
  }
]